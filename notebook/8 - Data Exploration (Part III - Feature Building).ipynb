{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration (Part III - Feature Building)\n",
    "## Intro\n",
    "Okay, I'm starting to get the hang of Spark. I've totally abused the Spark SQL capabilities so far, and it's been extremely user friendly doing all this in Jupyter. Honestly, it's not that large of a data set so I haven't had too much complexity navigating the fields so far. There are not many features to build, but going through the date and time section, I could think of a few there that we should pull out for deeper analysis.\n",
    "\n",
    "Let's set up our environment and load the data as we had it last time again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    require(['notebook/js/codecell'], function(codecell) {\n",
       "      // https://github.com/jupyter/notebook/issues/2453\n",
       "      codecell.CodeCell.options_default.highlight_modes['magic_text/x-sql'] = {'reg':[/^%read_sql/, /.*=\\s*%read_sql/,\n",
       "                                                                                      /^%%read_sql/]};\n",
       "      Jupyter.notebook.events.one('kernel_ready.Kernel', function(){\n",
       "          console.log('BBBBB');\n",
       "          Jupyter.notebook.get_cells().map(function(cell){\n",
       "              if (cell.cell_type == 'code'){ cell.auto_highlight(); } }) ;\n",
       "      });\n",
       "    });\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use findspark package to connect Jupyter to Spark shell\n",
    "import findspark\n",
    "findspark.init('/usr/lib/spark')\n",
    "\n",
    "# Load SparkSession object\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Load other libraries\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col, udf, count, isnan, lit, sum, coalesce, concat, to_date, to_timestamp, when, date_format, unix_timestamp\n",
    "from pyspark.sql.types import DateType\n",
    "from functools import reduce\n",
    "import pandas as pd\n",
    "\n",
    "# Initiate SparkSession as \"spark\"\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load sql_magic and connect to Spark\n",
    "%load_ext sql_magic\n",
    "%config SQL.conn_name = 'spark'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12 ms, sys: 0 ns, total: 12 ms\n",
      "Wall time: 52.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Read NYPD Complaint Data\n",
    "df = spark.read.csv(\n",
    "    \"s3n://2017edmfasatb/nypd_complaints/data/NYPD_Complaint_Data_Historic.csv\", \n",
    "    header = True, \n",
    "    inferSchema = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- COMPLAINT_NUMBER: integer (nullable = true)\n",
      " |-- COMPLAINT_START_DATE: string (nullable = true)\n",
      " |-- COMPLAINT_START_TIME: string (nullable = true)\n",
      " |-- COMPLAINT_END_DATE: string (nullable = true)\n",
      " |-- COMPLAINT_END_TIME: string (nullable = true)\n",
      " |-- REPORTED_DATE: string (nullable = true)\n",
      " |-- OFFENSE_ID: integer (nullable = true)\n",
      " |-- OFFENSE_DESCRIPTION: string (nullable = true)\n",
      " |-- OFFENSE_INTERNAL_CODE: integer (nullable = true)\n",
      " |-- OFFENSE_INTERNAL_DESCRIPTION: string (nullable = true)\n",
      " |-- OFFENSE_RESULT: string (nullable = true)\n",
      " |-- OFFENSE_LEVEL: string (nullable = true)\n",
      " |-- JURISDICTION: string (nullable = true)\n",
      " |-- BOROUGH: string (nullable = true)\n",
      " |-- PRECINCT: integer (nullable = true)\n",
      " |-- SPECIFIC_LOCATION: string (nullable = true)\n",
      " |-- PREMISE_DESCRIPTION: string (nullable = true)\n",
      " |-- PARK_NAME: string (nullable = true)\n",
      " |-- HOUSING_NAME: string (nullable = true)\n",
      " |-- X_COORD_NYC: integer (nullable = true)\n",
      " |-- Y_COORD_NYC: integer (nullable = true)\n",
      " |-- LAT: double (nullable = true)\n",
      " |-- LON: double (nullable = true)\n",
      " |-- LAT_LON: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "oldColumns = df.schema.names\n",
    "newColumns = [\n",
    "    'COMPLAINT_NUMBER',\n",
    "    'COMPLAINT_START_DATE',\n",
    "    'COMPLAINT_START_TIME',\n",
    "    'COMPLAINT_END_DATE',\n",
    "    'COMPLAINT_END_TIME',\n",
    "    'REPORTED_DATE',\n",
    "    'OFFENSE_ID',\n",
    "    'OFFENSE_DESCRIPTION',\n",
    "    'OFFENSE_INTERNAL_CODE',\n",
    "    'OFFENSE_INTERNAL_DESCRIPTION',\n",
    "    'OFFENSE_RESULT',\n",
    "    'OFFENSE_LEVEL',\n",
    "    'JURISDICTION',\n",
    "    'BOROUGH',\n",
    "    'PRECINCT',\n",
    "    'SPECIFIC_LOCATION',\n",
    "    'PREMISE_DESCRIPTION',\n",
    "    'PARK_NAME',\n",
    "    'HOUSING_NAME',\n",
    "    'X_COORD_NYC',\n",
    "    'Y_COORD_NYC',\n",
    "    'LAT',\n",
    "    'LON',\n",
    "    'LAT_LON'\n",
    "]\n",
    "\n",
    "df = reduce(lambda data, idx: data.withColumnRenamed(oldColumns[idx], newColumns[idx]), range(len(oldColumns)), df)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop rows with any NA values in the specified columns\n",
    "df_na_drop = df.na.drop(subset=[\n",
    "    'COMPLAINT_START_DATE',\n",
    "    'COMPLAINT_START_TIME',\n",
    "    'OFFENSE_DESCRIPTION',\n",
    "    'OFFENSE_RESULT',\n",
    "    'BOROUGH',\n",
    "    'PRECINCT',\n",
    "    'LAT',\n",
    "    'LON'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_na_drop = df_na_drop.withColumn('COMPLAINT_END_DATE', coalesce(df_na_drop['COMPLAINT_END_DATE'], df_na_drop['COMPLAINT_START_DATE']))\n",
    "df_na_drop = df_na_drop.withColumn('COMPLAINT_END_TIME', coalesce(df_na_drop['COMPLAINT_END_TIME'], df_na_drop['COMPLAINT_START_TIME'])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Combine date and time fields and create new timestamp field for COMPLAINT fields\n",
    "df_na_drop = df_na_drop.withColumn(\n",
    "    'COMPLAINT_START_TIMESTAMP', \n",
    "    to_timestamp(\n",
    "        concat(df_na_drop['COMPLAINT_START_DATE'], lit(' '), df_na_drop['COMPLAINT_START_TIME']),\n",
    "        'MM/dd/yyyy HH:mm:ss'\n",
    "    )\n",
    ")\n",
    "\n",
    "df_na_drop = df_na_drop.withColumn(\n",
    "    'COMPLAINT_END_TIMESTAMP', \n",
    "    to_timestamp(\n",
    "        concat(df_na_drop['COMPLAINT_END_DATE'], lit(' '), df_na_drop['COMPLAINT_END_TIME']),\n",
    "        'MM/dd/yyyy HH:mm:ss'\n",
    "    )\n",
    ")\n",
    "\n",
    "# Convert REPORTED_DATE\n",
    "df_na_drop = df_na_drop.withColumn(\n",
    "    'REPORTED_DATE_TIMESTAMP', \n",
    "    to_timestamp(\n",
    "        df_na_drop['REPORTED_DATE'],\n",
    "        'MM/dd/yyyy'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# List of crimes to keep\n",
    "crimes_to_keep = [\n",
    "    'PETIT LARCENY',\n",
    "    'HARRASSMENT 2',\n",
    "    'ASSAULT 3 & RELATED OFFENSES',\n",
    "    'CRIMINAL MISCHIEF & RELATED OF',\n",
    "    'GRAND LARCENY',\n",
    "    'OFF. AGNST PUB ORD SENSBLTY &',\n",
    "    'DANGEROUS DRUGS',\n",
    "    'ROBBERY',\n",
    "    'BURGLUARY',\n",
    "    'FELONY ASSAULT'\n",
    "]\n",
    "\n",
    "# Anything not in the list becomes 'OTHER'\n",
    "df_na_drop = df_na_drop.withColumn(\n",
    "    'OFFENSE_DESCRIPTION', \n",
    "    when(df_na_drop['OFFENSE_DESCRIPTION'].isin(crimes_to_keep), df_na_drop['OFFENSE_DESCRIPTION']).otherwise('OTHER')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# List of premises to keep\n",
    "premises_to_keep = [\n",
    "    'STREET',\n",
    "    'RESIDENCE - APT. HOUSE',\n",
    "    'RESIDENCE-HOUSE',\n",
    "    'RESIDENCE - PUBLIC HOUSING',\n",
    "    'COMMERCIAL BUILDING',\n",
    "    'DEPARTMENT STORE',\n",
    "    'TRANSIT - NYC SUBWAY',\n",
    "    'CHAIN STORE',\n",
    "    'PUBLIC SCHOOL',\n",
    "    'GROCERY/BODEGA',\n",
    "    'RESTAURANT/DINER',\n",
    "    'BAR/NIGHT CLUB',\n",
    "    'PARK/PLAYGROUND'\n",
    "]\n",
    "\n",
    "# Anything not in the list becomes 'OTHER'\n",
    "df_na_drop = df_na_drop.withColumn(\n",
    "    'PREMISE_DESCRIPTION', \n",
    "    when(df_na_drop['PREMISE_DESCRIPTION'].isin(premises_to_keep), df_na_drop['PREMISE_DESCRIPTION']).otherwise('OTHER')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add table to SQL Context\n",
    "df_na_drop.createOrReplaceTempView(\"df_na_drop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query started at 06:52:51 AM UTC; Query executed in 0.02 m"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMPLAINT_NUMBER</th>\n",
       "      <th>COMPLAINT_START_DATE</th>\n",
       "      <th>COMPLAINT_START_TIME</th>\n",
       "      <th>COMPLAINT_END_DATE</th>\n",
       "      <th>COMPLAINT_END_TIME</th>\n",
       "      <th>REPORTED_DATE</th>\n",
       "      <th>OFFENSE_ID</th>\n",
       "      <th>OFFENSE_DESCRIPTION</th>\n",
       "      <th>OFFENSE_INTERNAL_CODE</th>\n",
       "      <th>OFFENSE_INTERNAL_DESCRIPTION</th>\n",
       "      <th>...</th>\n",
       "      <th>PARK_NAME</th>\n",
       "      <th>HOUSING_NAME</th>\n",
       "      <th>X_COORD_NYC</th>\n",
       "      <th>Y_COORD_NYC</th>\n",
       "      <th>LAT</th>\n",
       "      <th>LON</th>\n",
       "      <th>LAT_LON</th>\n",
       "      <th>COMPLAINT_START_TIMESTAMP</th>\n",
       "      <th>COMPLAINT_END_TIMESTAMP</th>\n",
       "      <th>REPORTED_DATE_TIMESTAMP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101109527</td>\n",
       "      <td>12/31/2015</td>\n",
       "      <td>23:45:00</td>\n",
       "      <td>12/31/2015</td>\n",
       "      <td>23:45:00</td>\n",
       "      <td>12/31/2015</td>\n",
       "      <td>113</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>729.0</td>\n",
       "      <td>FORGERY,ETC.,UNCLASSIFIED-FELO</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1007314</td>\n",
       "      <td>241257</td>\n",
       "      <td>40.828848</td>\n",
       "      <td>-73.916661</td>\n",
       "      <td>(40.828848333, -73.916661142)</td>\n",
       "      <td>2015-12-31 23:45:00</td>\n",
       "      <td>2015-12-31 23:45:00</td>\n",
       "      <td>2015-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>153401121</td>\n",
       "      <td>12/31/2015</td>\n",
       "      <td>23:36:00</td>\n",
       "      <td>12/31/2015</td>\n",
       "      <td>23:36:00</td>\n",
       "      <td>12/31/2015</td>\n",
       "      <td>101</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1043991</td>\n",
       "      <td>193406</td>\n",
       "      <td>40.697338</td>\n",
       "      <td>-73.784557</td>\n",
       "      <td>(40.697338138, -73.784556739)</td>\n",
       "      <td>2015-12-31 23:36:00</td>\n",
       "      <td>2015-12-31 23:36:00</td>\n",
       "      <td>2015-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>569369778</td>\n",
       "      <td>12/31/2015</td>\n",
       "      <td>23:30:00</td>\n",
       "      <td>12/31/2015</td>\n",
       "      <td>23:30:00</td>\n",
       "      <td>12/31/2015</td>\n",
       "      <td>117</td>\n",
       "      <td>DANGEROUS DRUGS</td>\n",
       "      <td>503.0</td>\n",
       "      <td>CONTROLLED SUBSTANCE,INTENT TO</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>999463</td>\n",
       "      <td>231690</td>\n",
       "      <td>40.802607</td>\n",
       "      <td>-73.945052</td>\n",
       "      <td>(40.802606608, -73.945051911)</td>\n",
       "      <td>2015-12-31 23:30:00</td>\n",
       "      <td>2015-12-31 23:30:00</td>\n",
       "      <td>2015-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>968417082</td>\n",
       "      <td>12/31/2015</td>\n",
       "      <td>23:30:00</td>\n",
       "      <td>12/31/2015</td>\n",
       "      <td>23:30:00</td>\n",
       "      <td>12/31/2015</td>\n",
       "      <td>344</td>\n",
       "      <td>ASSAULT 3 &amp; RELATED OFFENSES</td>\n",
       "      <td>101.0</td>\n",
       "      <td>ASSAULT 3</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1060183</td>\n",
       "      <td>177862</td>\n",
       "      <td>40.654549</td>\n",
       "      <td>-73.726339</td>\n",
       "      <td>(40.654549444, -73.726338791)</td>\n",
       "      <td>2015-12-31 23:30:00</td>\n",
       "      <td>2015-12-31 23:30:00</td>\n",
       "      <td>2015-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>641637920</td>\n",
       "      <td>12/31/2015</td>\n",
       "      <td>23:25:00</td>\n",
       "      <td>12/31/2015</td>\n",
       "      <td>23:30:00</td>\n",
       "      <td>12/31/2015</td>\n",
       "      <td>344</td>\n",
       "      <td>ASSAULT 3 &amp; RELATED OFFENSES</td>\n",
       "      <td>101.0</td>\n",
       "      <td>ASSAULT 3</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>987606</td>\n",
       "      <td>208148</td>\n",
       "      <td>40.738002</td>\n",
       "      <td>-73.987891</td>\n",
       "      <td>(40.7380024, -73.98789129)</td>\n",
       "      <td>2015-12-31 23:25:00</td>\n",
       "      <td>2015-12-31 23:30:00</td>\n",
       "      <td>2015-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>365661343</td>\n",
       "      <td>12/31/2015</td>\n",
       "      <td>23:18:00</td>\n",
       "      <td>12/31/2015</td>\n",
       "      <td>23:25:00</td>\n",
       "      <td>12/31/2015</td>\n",
       "      <td>106</td>\n",
       "      <td>FELONY ASSAULT</td>\n",
       "      <td>109.0</td>\n",
       "      <td>ASSAULT 2,1,UNCLASSIFIED</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>996149</td>\n",
       "      <td>181562</td>\n",
       "      <td>40.665023</td>\n",
       "      <td>-73.957111</td>\n",
       "      <td>(40.665022689, -73.957110763)</td>\n",
       "      <td>2015-12-31 23:18:00</td>\n",
       "      <td>2015-12-31 23:25:00</td>\n",
       "      <td>2015-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>608231454</td>\n",
       "      <td>12/31/2015</td>\n",
       "      <td>23:15:00</td>\n",
       "      <td>12/31/2015</td>\n",
       "      <td>23:15:00</td>\n",
       "      <td>12/31/2015</td>\n",
       "      <td>235</td>\n",
       "      <td>DANGEROUS DRUGS</td>\n",
       "      <td>511.0</td>\n",
       "      <td>CONTROLLED SUBSTANCE, POSSESSI</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>987373</td>\n",
       "      <td>201662</td>\n",
       "      <td>40.720200</td>\n",
       "      <td>-73.988735</td>\n",
       "      <td>(40.720199996, -73.988735082)</td>\n",
       "      <td>2015-12-31 23:15:00</td>\n",
       "      <td>2015-12-31 23:15:00</td>\n",
       "      <td>2015-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>265023856</td>\n",
       "      <td>12/31/2015</td>\n",
       "      <td>23:15:00</td>\n",
       "      <td>12/31/2015</td>\n",
       "      <td>23:15:00</td>\n",
       "      <td>12/31/2015</td>\n",
       "      <td>118</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>792.0</td>\n",
       "      <td>WEAPONS POSSESSION 1 &amp; 2</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1009041</td>\n",
       "      <td>247401</td>\n",
       "      <td>40.845707</td>\n",
       "      <td>-73.910398</td>\n",
       "      <td>(40.845707148, -73.910398033)</td>\n",
       "      <td>2015-12-31 23:15:00</td>\n",
       "      <td>2015-12-31 23:15:00</td>\n",
       "      <td>2015-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>989238731</td>\n",
       "      <td>12/31/2015</td>\n",
       "      <td>23:15:00</td>\n",
       "      <td>12/31/2015</td>\n",
       "      <td>23:30:00</td>\n",
       "      <td>12/31/2015</td>\n",
       "      <td>344</td>\n",
       "      <td>ASSAULT 3 &amp; RELATED OFFENSES</td>\n",
       "      <td>101.0</td>\n",
       "      <td>ASSAULT 3</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1014154</td>\n",
       "      <td>251416</td>\n",
       "      <td>40.856711</td>\n",
       "      <td>-73.891900</td>\n",
       "      <td>(40.856711291, -73.891899956)</td>\n",
       "      <td>2015-12-31 23:15:00</td>\n",
       "      <td>2015-12-31 23:30:00</td>\n",
       "      <td>2015-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>415095955</td>\n",
       "      <td>12/31/2015</td>\n",
       "      <td>23:10:00</td>\n",
       "      <td>12/31/2015</td>\n",
       "      <td>23:10:00</td>\n",
       "      <td>12/31/2015</td>\n",
       "      <td>341</td>\n",
       "      <td>PETIT LARCENY</td>\n",
       "      <td>338.0</td>\n",
       "      <td>LARCENY,PETIT FROM BUILDING,UN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>994327</td>\n",
       "      <td>218211</td>\n",
       "      <td>40.765618</td>\n",
       "      <td>-73.963623</td>\n",
       "      <td>(40.765617688, -73.96362342)</td>\n",
       "      <td>2015-12-31 23:10:00</td>\n",
       "      <td>2015-12-31 23:10:00</td>\n",
       "      <td>2015-12-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   COMPLAINT_NUMBER COMPLAINT_START_DATE COMPLAINT_START_TIME  \\\n",
       "0         101109527           12/31/2015             23:45:00   \n",
       "1         153401121           12/31/2015             23:36:00   \n",
       "2         569369778           12/31/2015             23:30:00   \n",
       "3         968417082           12/31/2015             23:30:00   \n",
       "4         641637920           12/31/2015             23:25:00   \n",
       "5         365661343           12/31/2015             23:18:00   \n",
       "6         608231454           12/31/2015             23:15:00   \n",
       "7         265023856           12/31/2015             23:15:00   \n",
       "8         989238731           12/31/2015             23:15:00   \n",
       "9         415095955           12/31/2015             23:10:00   \n",
       "\n",
       "  COMPLAINT_END_DATE COMPLAINT_END_TIME REPORTED_DATE  OFFENSE_ID  \\\n",
       "0         12/31/2015           23:45:00    12/31/2015         113   \n",
       "1         12/31/2015           23:36:00    12/31/2015         101   \n",
       "2         12/31/2015           23:30:00    12/31/2015         117   \n",
       "3         12/31/2015           23:30:00    12/31/2015         344   \n",
       "4         12/31/2015           23:30:00    12/31/2015         344   \n",
       "5         12/31/2015           23:25:00    12/31/2015         106   \n",
       "6         12/31/2015           23:15:00    12/31/2015         235   \n",
       "7         12/31/2015           23:15:00    12/31/2015         118   \n",
       "8         12/31/2015           23:30:00    12/31/2015         344   \n",
       "9         12/31/2015           23:10:00    12/31/2015         341   \n",
       "\n",
       "            OFFENSE_DESCRIPTION  OFFENSE_INTERNAL_CODE  \\\n",
       "0                         OTHER                  729.0   \n",
       "1                         OTHER                    NaN   \n",
       "2               DANGEROUS DRUGS                  503.0   \n",
       "3  ASSAULT 3 & RELATED OFFENSES                  101.0   \n",
       "4  ASSAULT 3 & RELATED OFFENSES                  101.0   \n",
       "5                FELONY ASSAULT                  109.0   \n",
       "6               DANGEROUS DRUGS                  511.0   \n",
       "7                         OTHER                  792.0   \n",
       "8  ASSAULT 3 & RELATED OFFENSES                  101.0   \n",
       "9                 PETIT LARCENY                  338.0   \n",
       "\n",
       "     OFFENSE_INTERNAL_DESCRIPTION           ...            PARK_NAME  \\\n",
       "0  FORGERY,ETC.,UNCLASSIFIED-FELO           ...                 None   \n",
       "1                            None           ...                 None   \n",
       "2  CONTROLLED SUBSTANCE,INTENT TO           ...                 None   \n",
       "3                       ASSAULT 3           ...                 None   \n",
       "4                       ASSAULT 3           ...                 None   \n",
       "5        ASSAULT 2,1,UNCLASSIFIED           ...                 None   \n",
       "6  CONTROLLED SUBSTANCE, POSSESSI           ...                 None   \n",
       "7        WEAPONS POSSESSION 1 & 2           ...                 None   \n",
       "8                       ASSAULT 3           ...                 None   \n",
       "9  LARCENY,PETIT FROM BUILDING,UN           ...                 None   \n",
       "\n",
       "  HOUSING_NAME X_COORD_NYC Y_COORD_NYC        LAT        LON  \\\n",
       "0         None     1007314      241257  40.828848 -73.916661   \n",
       "1         None     1043991      193406  40.697338 -73.784557   \n",
       "2         None      999463      231690  40.802607 -73.945052   \n",
       "3         None     1060183      177862  40.654549 -73.726339   \n",
       "4         None      987606      208148  40.738002 -73.987891   \n",
       "5         None      996149      181562  40.665023 -73.957111   \n",
       "6         None      987373      201662  40.720200 -73.988735   \n",
       "7         None     1009041      247401  40.845707 -73.910398   \n",
       "8         None     1014154      251416  40.856711 -73.891900   \n",
       "9         None      994327      218211  40.765618 -73.963623   \n",
       "\n",
       "                         LAT_LON COMPLAINT_START_TIMESTAMP  \\\n",
       "0  (40.828848333, -73.916661142)       2015-12-31 23:45:00   \n",
       "1  (40.697338138, -73.784556739)       2015-12-31 23:36:00   \n",
       "2  (40.802606608, -73.945051911)       2015-12-31 23:30:00   \n",
       "3  (40.654549444, -73.726338791)       2015-12-31 23:30:00   \n",
       "4     (40.7380024, -73.98789129)       2015-12-31 23:25:00   \n",
       "5  (40.665022689, -73.957110763)       2015-12-31 23:18:00   \n",
       "6  (40.720199996, -73.988735082)       2015-12-31 23:15:00   \n",
       "7  (40.845707148, -73.910398033)       2015-12-31 23:15:00   \n",
       "8  (40.856711291, -73.891899956)       2015-12-31 23:15:00   \n",
       "9   (40.765617688, -73.96362342)       2015-12-31 23:10:00   \n",
       "\n",
       "  COMPLAINT_END_TIMESTAMP  REPORTED_DATE_TIMESTAMP  \n",
       "0     2015-12-31 23:45:00               2015-12-31  \n",
       "1     2015-12-31 23:36:00               2015-12-31  \n",
       "2     2015-12-31 23:30:00               2015-12-31  \n",
       "3     2015-12-31 23:30:00               2015-12-31  \n",
       "4     2015-12-31 23:30:00               2015-12-31  \n",
       "5     2015-12-31 23:25:00               2015-12-31  \n",
       "6     2015-12-31 23:15:00               2015-12-31  \n",
       "7     2015-12-31 23:15:00               2015-12-31  \n",
       "8     2015-12-31 23:30:00               2015-12-31  \n",
       "9     2015-12-31 23:10:00               2015-12-31  \n",
       "\n",
       "[10 rows x 27 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%read_sql\n",
    "SELECT * FROM df_na_drop LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timestamp Columns\n",
    "Remember, in the last post, we cleaned up the **COMPLAINT\\_START**, **COMPLAINT\\_END**, and **REPORTED\\_DATE** columns. Through these 3 fields, alone, I can probably think of 5 or 6 features off the top of my head that we should build:\n",
    "- Year, Month, Day, Day Of Week, and Hour of each of these dates (no hour for REPORTED\\_DATE)\n",
    "- Flag of whether incident spanned any amount of time (vs a one-time incident where COMPLAINT\\_START = COMPLAINT\\_END)\n",
    "- Length of the incident, if not one-time\n",
    "- How far after the incident started and ended was the event reported\n",
    "\n",
    "That will actually give us 18 new features haha... actually, I'm not going to do the datetime fields breakdown of COMPLAINT\\_END or REPORTED\\_DATE, these don't really matter as much as when the incident actually started. This leaves us with 9 new features - I'll take it.\n",
    "\n",
    "### Date / Time Fields For COMPLAINT\\_START"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set UDFs to extract specific parts of date and time\n",
    "extract_year =  udf(lambda x: x.year)\n",
    "extract_month =  udf(lambda x: x.month)\n",
    "extract_day =  udf(lambda x: x.day)\n",
    "extract_hour =  udf(lambda x: x.hour)\n",
    "\n",
    "# Perform transformation\n",
    "df_na_drop = df_na_drop.withColumn('COMPLAINT_START_TIMESTAMP_YEAR', extract_year(col('COMPLAINT_START_TIMESTAMP')))\n",
    "df_na_drop = df_na_drop.withColumn('COMPLAINT_START_TIMESTAMP_MONTH', extract_month(col('COMPLAINT_START_TIMESTAMP')))\n",
    "df_na_drop = df_na_drop.withColumn('COMPLAINT_START_TIMESTAMP_DAY', extract_day(col('COMPLAINT_START_TIMESTAMP')))\n",
    "df_na_drop = df_na_drop.withColumn('COMPLAINT_START_TIMESTAMP_WEEKDAY', date_format(col('COMPLAINT_START_TIMESTAMP'), 'E'))\n",
    "df_na_drop = df_na_drop.withColumn('COMPLAINT_START_TIMESTAMP_HOUR', extract_hour(col('COMPLAINT_START_TIMESTAMP')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add table to SQL Context\n",
    "df_na_drop.createOrReplaceTempView(\"df_na_drop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query started at 06:52:52 AM UTC; Query executed in 0.04 m"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMPLAINT_START_TIMESTAMP</th>\n",
       "      <th>COMPLAINT_START_TIMESTAMP_YEAR</th>\n",
       "      <th>COMPLAINT_START_TIMESTAMP_MONTH</th>\n",
       "      <th>COMPLAINT_START_TIMESTAMP_DAY</th>\n",
       "      <th>COMPLAINT_START_TIMESTAMP_WEEKDAY</th>\n",
       "      <th>COMPLAINT_START_TIMESTAMP_HOUR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-12-31 23:45:00</td>\n",
       "      <td>2015</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>Thu</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-12-31 23:36:00</td>\n",
       "      <td>2015</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>Thu</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-12-31 23:30:00</td>\n",
       "      <td>2015</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>Thu</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-12-31 23:30:00</td>\n",
       "      <td>2015</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>Thu</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-12-31 23:25:00</td>\n",
       "      <td>2015</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>Thu</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2015-12-31 23:18:00</td>\n",
       "      <td>2015</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>Thu</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2015-12-31 23:15:00</td>\n",
       "      <td>2015</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>Thu</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2015-12-31 23:15:00</td>\n",
       "      <td>2015</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>Thu</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2015-12-31 23:15:00</td>\n",
       "      <td>2015</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>Thu</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2015-12-31 23:10:00</td>\n",
       "      <td>2015</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>Thu</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  COMPLAINT_START_TIMESTAMP COMPLAINT_START_TIMESTAMP_YEAR  \\\n",
       "0       2015-12-31 23:45:00                           2015   \n",
       "1       2015-12-31 23:36:00                           2015   \n",
       "2       2015-12-31 23:30:00                           2015   \n",
       "3       2015-12-31 23:30:00                           2015   \n",
       "4       2015-12-31 23:25:00                           2015   \n",
       "5       2015-12-31 23:18:00                           2015   \n",
       "6       2015-12-31 23:15:00                           2015   \n",
       "7       2015-12-31 23:15:00                           2015   \n",
       "8       2015-12-31 23:15:00                           2015   \n",
       "9       2015-12-31 23:10:00                           2015   \n",
       "\n",
       "  COMPLAINT_START_TIMESTAMP_MONTH COMPLAINT_START_TIMESTAMP_DAY  \\\n",
       "0                              12                            31   \n",
       "1                              12                            31   \n",
       "2                              12                            31   \n",
       "3                              12                            31   \n",
       "4                              12                            31   \n",
       "5                              12                            31   \n",
       "6                              12                            31   \n",
       "7                              12                            31   \n",
       "8                              12                            31   \n",
       "9                              12                            31   \n",
       "\n",
       "  COMPLAINT_START_TIMESTAMP_WEEKDAY COMPLAINT_START_TIMESTAMP_HOUR  \n",
       "0                               Thu                             23  \n",
       "1                               Thu                             23  \n",
       "2                               Thu                             23  \n",
       "3                               Thu                             23  \n",
       "4                               Thu                             23  \n",
       "5                               Thu                             23  \n",
       "6                               Thu                             23  \n",
       "7                               Thu                             23  \n",
       "8                               Thu                             23  \n",
       "9                               Thu                             23  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%read_sql\n",
    "SELECT \n",
    "    COMPLAINT_START_TIMESTAMP,\n",
    "    COMPLAINT_START_TIMESTAMP_YEAR,\n",
    "    COMPLAINT_START_TIMESTAMP_MONTH,\n",
    "    COMPLAINT_START_TIMESTAMP_DAY,\n",
    "    COMPLAINT_START_TIMESTAMP_WEEKDAY,\n",
    "    COMPLAINT_START_TIMESTAMP_HOUR\n",
    "FROM df_na_drop \n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect. Let's move onto the next task.\n",
    "\n",
    "### Incident Length Flag & Incident Length\n",
    "Very simple here. I want to\n",
    "1. I want to calculate how long the incident lasted\n",
    "2. If the incident was a single instance in time, I want to flag it in some way in another column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the difference between start and end, expressed in minutes\n",
    "df_na_drop = df_na_drop.withColumn(\n",
    "    'COMPLAINT_LENGTH', \n",
    "    (unix_timestamp(df_na_drop['COMPLAINT_END_TIMESTAMP']) - unix_timestamp(df_na_drop['COMPLAINT_START_TIMESTAMP']))/60\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add table to SQL Context\n",
    "df_na_drop.createOrReplaceTempView(\"df_na_drop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query started at 06:52:55 AM UTC; Query executed in 0.01 m"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMPLAINT_START_TIMESTAMP</th>\n",
       "      <th>COMPLAINT_END_TIMESTAMP</th>\n",
       "      <th>COMPLAINT_LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-12-31 23:45:00</td>\n",
       "      <td>2015-12-31 23:45:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-12-31 23:36:00</td>\n",
       "      <td>2015-12-31 23:36:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-12-31 23:30:00</td>\n",
       "      <td>2015-12-31 23:30:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-12-31 23:30:00</td>\n",
       "      <td>2015-12-31 23:30:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-12-31 23:25:00</td>\n",
       "      <td>2015-12-31 23:30:00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2015-12-31 23:18:00</td>\n",
       "      <td>2015-12-31 23:25:00</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2015-12-31 23:15:00</td>\n",
       "      <td>2015-12-31 23:15:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2015-12-31 23:15:00</td>\n",
       "      <td>2015-12-31 23:15:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2015-12-31 23:15:00</td>\n",
       "      <td>2015-12-31 23:30:00</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2015-12-31 23:10:00</td>\n",
       "      <td>2015-12-31 23:10:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  COMPLAINT_START_TIMESTAMP COMPLAINT_END_TIMESTAMP  COMPLAINT_LENGTH\n",
       "0       2015-12-31 23:45:00     2015-12-31 23:45:00               0.0\n",
       "1       2015-12-31 23:36:00     2015-12-31 23:36:00               0.0\n",
       "2       2015-12-31 23:30:00     2015-12-31 23:30:00               0.0\n",
       "3       2015-12-31 23:30:00     2015-12-31 23:30:00               0.0\n",
       "4       2015-12-31 23:25:00     2015-12-31 23:30:00               5.0\n",
       "5       2015-12-31 23:18:00     2015-12-31 23:25:00               7.0\n",
       "6       2015-12-31 23:15:00     2015-12-31 23:15:00               0.0\n",
       "7       2015-12-31 23:15:00     2015-12-31 23:15:00               0.0\n",
       "8       2015-12-31 23:15:00     2015-12-31 23:30:00              15.0\n",
       "9       2015-12-31 23:10:00     2015-12-31 23:10:00               0.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%read_sql\n",
    "SELECT \n",
    "    COMPLAINT_START_TIMESTAMP,\n",
    "    COMPLAINT_END_TIMESTAMP,\n",
    "    COMPLAINT_LENGTH\n",
    "FROM df_na_drop \n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If COMPLAINT_LENGTH = 0, we flag with a new boolean column COMPLAINT_LENGTH_ZERO_TIME\n",
    "df_na_drop = df_na_drop.withColumn(\n",
    "    'COMPLAINT_LENGTH_ZERO_TIME', when(df_na_drop['COMPLAINT_LENGTH'] == 0, True).otherwise(False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add table to SQL Context\n",
    "df_na_drop.createOrReplaceTempView(\"df_na_drop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query started at 06:52:56 AM UTC; Query executed in 0.01 m"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMPLAINT_LENGTH</th>\n",
       "      <th>COMPLAINT_LENGTH_ZERO_TIME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>15.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   COMPLAINT_LENGTH  COMPLAINT_LENGTH_ZERO_TIME\n",
       "0               0.0                        True\n",
       "1               0.0                        True\n",
       "2               0.0                        True\n",
       "3               0.0                        True\n",
       "4               5.0                       False\n",
       "5               7.0                       False\n",
       "6               0.0                        True\n",
       "7               0.0                        True\n",
       "8              15.0                       False\n",
       "9               0.0                        True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%read_sql\n",
    "SELECT \n",
    "    COMPLAINT_LENGTH,\n",
    "    COMPLAINT_LENGTH_ZERO_TIME\n",
    "FROM df_na_drop \n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lag Time Between Incident And Report Date\n",
    "This one is kinda weird because report date doesn't have a time component in this dataset, only a date. We'll take the difference between both the COMPLAINT\\_START and COMPLAINT\\_END dates with the REPORTED\\_DATE. My assumption here is that the incident always happens first and then it is reported when it is completely finished. This may be a wrong assumption, but I'll make this assumption for the sake of the subtraction and we will allow negative values if the complaint can be reported before the incident actually finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the difference between start and reported, expressed in days\n",
    "df_na_drop = df_na_drop.withColumn(\n",
    "    'COMPLAINT_START_REPORTED_LAG', \n",
    "    (unix_timestamp(df_na_drop['REPORTED_DATE_TIMESTAMP']) - unix_timestamp(to_date(df_na_drop['COMPLAINT_START_TIMESTAMP'])))/60/60/24\n",
    ")\n",
    "\n",
    "# Take the difference between end and reported, expressed in days\n",
    "df_na_drop = df_na_drop.withColumn(\n",
    "    'COMPLAINT_END_REPORTED_LAG', \n",
    "    (unix_timestamp(df_na_drop['REPORTED_DATE_TIMESTAMP']) - unix_timestamp(to_date(df_na_drop['COMPLAINT_END_TIMESTAMP'])))/60/60/24\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add table to SQL Context\n",
    "df_na_drop.createOrReplaceTempView(\"df_na_drop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query started at 06:52:57 AM UTC; Query executed in 0.01 m"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMPLAINT_START_TIMESTAMP</th>\n",
       "      <th>COMPLAINT_END_TIMESTAMP</th>\n",
       "      <th>REPORTED_DATE_TIMESTAMP</th>\n",
       "      <th>COMPLAINT_START_REPORTED_LAG</th>\n",
       "      <th>COMPLAINT_END_REPORTED_LAG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-12-31 23:45:00</td>\n",
       "      <td>2015-12-31 23:45:00</td>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-12-31 23:36:00</td>\n",
       "      <td>2015-12-31 23:36:00</td>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-12-31 23:30:00</td>\n",
       "      <td>2015-12-31 23:30:00</td>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-12-31 23:30:00</td>\n",
       "      <td>2015-12-31 23:30:00</td>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-12-31 23:25:00</td>\n",
       "      <td>2015-12-31 23:30:00</td>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2015-12-31 23:18:00</td>\n",
       "      <td>2015-12-31 23:25:00</td>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2015-12-31 23:15:00</td>\n",
       "      <td>2015-12-31 23:15:00</td>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2015-12-31 23:15:00</td>\n",
       "      <td>2015-12-31 23:15:00</td>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2015-12-31 23:15:00</td>\n",
       "      <td>2015-12-31 23:30:00</td>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2015-12-31 23:10:00</td>\n",
       "      <td>2015-12-31 23:10:00</td>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  COMPLAINT_START_TIMESTAMP COMPLAINT_END_TIMESTAMP REPORTED_DATE_TIMESTAMP  \\\n",
       "0       2015-12-31 23:45:00     2015-12-31 23:45:00              2015-12-31   \n",
       "1       2015-12-31 23:36:00     2015-12-31 23:36:00              2015-12-31   \n",
       "2       2015-12-31 23:30:00     2015-12-31 23:30:00              2015-12-31   \n",
       "3       2015-12-31 23:30:00     2015-12-31 23:30:00              2015-12-31   \n",
       "4       2015-12-31 23:25:00     2015-12-31 23:30:00              2015-12-31   \n",
       "5       2015-12-31 23:18:00     2015-12-31 23:25:00              2015-12-31   \n",
       "6       2015-12-31 23:15:00     2015-12-31 23:15:00              2015-12-31   \n",
       "7       2015-12-31 23:15:00     2015-12-31 23:15:00              2015-12-31   \n",
       "8       2015-12-31 23:15:00     2015-12-31 23:30:00              2015-12-31   \n",
       "9       2015-12-31 23:10:00     2015-12-31 23:10:00              2015-12-31   \n",
       "\n",
       "   COMPLAINT_START_REPORTED_LAG  COMPLAINT_END_REPORTED_LAG  \n",
       "0                           0.0                         0.0  \n",
       "1                           0.0                         0.0  \n",
       "2                           0.0                         0.0  \n",
       "3                           0.0                         0.0  \n",
       "4                           0.0                         0.0  \n",
       "5                           0.0                         0.0  \n",
       "6                           0.0                         0.0  \n",
       "7                           0.0                         0.0  \n",
       "8                           0.0                         0.0  \n",
       "9                           0.0                         0.0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%read_sql\n",
    "SELECT \n",
    "    COMPLAINT_START_TIMESTAMP,\n",
    "    COMPLAINT_END_TIMESTAMP,\n",
    "    REPORTED_DATE_TIMESTAMP,\n",
    "    COMPLAINT_START_REPORTED_LAG,\n",
    "    COMPLAINT_END_REPORTED_LAG\n",
    "FROM df_na_drop \n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay. I think we have our final dataframe! I'm not really sure what other features I can engineer at this point without getting deeper into actual analysis, so let's save this dataframe back out to S3 as a temporary store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- COMPLAINT_NUMBER: integer (nullable = true)\n",
      " |-- COMPLAINT_START_DATE: string (nullable = true)\n",
      " |-- COMPLAINT_START_TIME: string (nullable = true)\n",
      " |-- COMPLAINT_END_DATE: string (nullable = true)\n",
      " |-- COMPLAINT_END_TIME: string (nullable = true)\n",
      " |-- REPORTED_DATE: string (nullable = true)\n",
      " |-- OFFENSE_ID: integer (nullable = true)\n",
      " |-- OFFENSE_DESCRIPTION: string (nullable = true)\n",
      " |-- OFFENSE_INTERNAL_CODE: integer (nullable = true)\n",
      " |-- OFFENSE_INTERNAL_DESCRIPTION: string (nullable = true)\n",
      " |-- OFFENSE_RESULT: string (nullable = true)\n",
      " |-- OFFENSE_LEVEL: string (nullable = true)\n",
      " |-- JURISDICTION: string (nullable = true)\n",
      " |-- BOROUGH: string (nullable = true)\n",
      " |-- PRECINCT: integer (nullable = true)\n",
      " |-- SPECIFIC_LOCATION: string (nullable = true)\n",
      " |-- PREMISE_DESCRIPTION: string (nullable = true)\n",
      " |-- PARK_NAME: string (nullable = true)\n",
      " |-- HOUSING_NAME: string (nullable = true)\n",
      " |-- X_COORD_NYC: integer (nullable = true)\n",
      " |-- Y_COORD_NYC: integer (nullable = true)\n",
      " |-- LAT: double (nullable = true)\n",
      " |-- LON: double (nullable = true)\n",
      " |-- LAT_LON: string (nullable = true)\n",
      " |-- COMPLAINT_START_TIMESTAMP: timestamp (nullable = true)\n",
      " |-- COMPLAINT_END_TIMESTAMP: timestamp (nullable = true)\n",
      " |-- REPORTED_DATE_TIMESTAMP: timestamp (nullable = true)\n",
      " |-- COMPLAINT_START_TIMESTAMP_YEAR: string (nullable = true)\n",
      " |-- COMPLAINT_START_TIMESTAMP_MONTH: string (nullable = true)\n",
      " |-- COMPLAINT_START_TIMESTAMP_DAY: string (nullable = true)\n",
      " |-- COMPLAINT_START_TIMESTAMP_WEEKDAY: string (nullable = true)\n",
      " |-- COMPLAINT_START_TIMESTAMP_HOUR: string (nullable = true)\n",
      " |-- COMPLAINT_LENGTH: double (nullable = true)\n",
      " |-- COMPLAINT_LENGTH_ZERO_TIME: boolean (nullable = false)\n",
      " |-- COMPLAINT_START_REPORTED_LAG: double (nullable = true)\n",
      " |-- COMPLAINT_END_REPORTED_LAG: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_na_drop.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_na_drop[[\n",
    "    'COMPLAINT_NUMBER', \n",
    "    'COMPLAINT_START_TIMESTAMP',\n",
    "    'COMPLAINT_END_TIMESTAMP',\n",
    "    'REPORTED_DATE_TIMESTAMP',\n",
    "    'COMPLAINT_START_TIMESTAMP_YEAR',\n",
    "    'COMPLAINT_START_TIMESTAMP_MONTH',\n",
    "    'COMPLAINT_START_TIMESTAMP_DAY',\n",
    "    'COMPLAINT_START_TIMESTAMP_WEEKDAY',\n",
    "    'COMPLAINT_START_TIMESTAMP_HOUR',\n",
    "    'COMPLAINT_LENGTH',\n",
    "    'COMPLAINT_LENGTH_ZERO_TIME',\n",
    "    'COMPLAINT_START_REPORTED_LAG',\n",
    "    'COMPLAINT_END_REPORTED_LAG',\n",
    "    'OFFENSE_DESCRIPTION',\n",
    "    'OFFENSE_RESULT',\n",
    "    'OFFENSE_LEVEL',\n",
    "    'JURISDICTION',\n",
    "    'BOROUGH',\n",
    "    'PRECINCT',\n",
    "    'SPECIFIC_LOCATION',\n",
    "    'PREMISE_DESCRIPTION',\n",
    "    'LAT',\n",
    "    'LON'\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o266.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:215)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:173)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:173)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:173)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:145)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.datasources.DataSource.writeInFileFormat(DataSource.scala:438)\n\tat org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:474)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:610)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:233)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:217)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:509)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 8.0 failed 4 times, most recent failure: Lost task 8.3 in stage 8.0 (TID 33, ip-10-0-0-154.ec2.internal, executor 2): org.apache.spark.SparkException: Task failed while writing rows\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:272)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$apply$mcV$sp$1.apply(FileFormatWriter.scala:191)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$apply$mcV$sp$1.apply(FileFormatWriter.scala:190)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1504379514127_0006/container_1504379514127_0006_01_000005/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1504379514127_0006/container_1504379514127_0006_01_000005/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1504379514127_0006/container_1504379514127_0006_01_000005/pyspark.zip/pyspark/serializers.py\", line 220, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1504379514127_0006/container_1504379514127_0006_01_000005/pyspark.zip/pyspark/serializers.py\", line 138, in dump_stream\n    for obj in iterator:\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1504379514127_0006/container_1504379514127_0006_01_000005/pyspark.zip/pyspark/serializers.py\", line 209, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1504379514127_0006/container_1504379514127_0006_01_000005/pyspark.zip/pyspark/worker.py\", line 71, in <lambda>\n    return lambda *a: f(*a)\n  File \"<ipython-input-11-ca9fdfbab274>\", line 2, in <lambda>\nAttributeError: 'NoneType' object has no attribute 'year'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:156)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:152)\n\tat org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:40)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:395)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:315)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:258)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:256)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1375)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:261)\n\t... 8 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1569)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1557)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1556)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1556)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:815)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:815)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:815)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1784)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1739)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1728)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:631)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:188)\n\t... 45 more\nCaused by: org.apache.spark.SparkException: Task failed while writing rows\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:272)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$apply$mcV$sp$1.apply(FileFormatWriter.scala:191)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$apply$mcV$sp$1.apply(FileFormatWriter.scala:190)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1504379514127_0006/container_1504379514127_0006_01_000005/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1504379514127_0006/container_1504379514127_0006_01_000005/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1504379514127_0006/container_1504379514127_0006_01_000005/pyspark.zip/pyspark/serializers.py\", line 220, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1504379514127_0006/container_1504379514127_0006_01_000005/pyspark.zip/pyspark/serializers.py\", line 138, in dump_stream\n    for obj in iterator:\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1504379514127_0006/container_1504379514127_0006_01_000005/pyspark.zip/pyspark/serializers.py\", line 209, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1504379514127_0006/container_1504379514127_0006_01_000005/pyspark.zip/pyspark/worker.py\", line 71, in <lambda>\n    return lambda *a: f(*a)\n  File \"<ipython-input-11-ca9fdfbab274>\", line 2, in <lambda>\nAttributeError: 'NoneType' object has no attribute 'year'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:156)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:152)\n\tat org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:40)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:395)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:315)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:258)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:256)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1375)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:261)\n\t... 8 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-3c8aba718f02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Save CSV back to S3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_clean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m's3n://2017edmfasatb/nypd_complaints/data/df_clean.parquet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m    689\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o266.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:215)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:173)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:173)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:173)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:145)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.datasources.DataSource.writeInFileFormat(DataSource.scala:438)\n\tat org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:474)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:610)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:233)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:217)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:509)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 8.0 failed 4 times, most recent failure: Lost task 8.3 in stage 8.0 (TID 33, ip-10-0-0-154.ec2.internal, executor 2): org.apache.spark.SparkException: Task failed while writing rows\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:272)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$apply$mcV$sp$1.apply(FileFormatWriter.scala:191)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$apply$mcV$sp$1.apply(FileFormatWriter.scala:190)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1504379514127_0006/container_1504379514127_0006_01_000005/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1504379514127_0006/container_1504379514127_0006_01_000005/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1504379514127_0006/container_1504379514127_0006_01_000005/pyspark.zip/pyspark/serializers.py\", line 220, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1504379514127_0006/container_1504379514127_0006_01_000005/pyspark.zip/pyspark/serializers.py\", line 138, in dump_stream\n    for obj in iterator:\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1504379514127_0006/container_1504379514127_0006_01_000005/pyspark.zip/pyspark/serializers.py\", line 209, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1504379514127_0006/container_1504379514127_0006_01_000005/pyspark.zip/pyspark/worker.py\", line 71, in <lambda>\n    return lambda *a: f(*a)\n  File \"<ipython-input-11-ca9fdfbab274>\", line 2, in <lambda>\nAttributeError: 'NoneType' object has no attribute 'year'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:156)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:152)\n\tat org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:40)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:395)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:315)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:258)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:256)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1375)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:261)\n\t... 8 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1569)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1557)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1556)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1556)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:815)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:815)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:815)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1784)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1739)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1728)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:631)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:188)\n\t... 45 more\nCaused by: org.apache.spark.SparkException: Task failed while writing rows\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:272)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$apply$mcV$sp$1.apply(FileFormatWriter.scala:191)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$apply$mcV$sp$1.apply(FileFormatWriter.scala:190)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1504379514127_0006/container_1504379514127_0006_01_000005/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1504379514127_0006/container_1504379514127_0006_01_000005/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1504379514127_0006/container_1504379514127_0006_01_000005/pyspark.zip/pyspark/serializers.py\", line 220, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1504379514127_0006/container_1504379514127_0006_01_000005/pyspark.zip/pyspark/serializers.py\", line 138, in dump_stream\n    for obj in iterator:\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1504379514127_0006/container_1504379514127_0006_01_000005/pyspark.zip/pyspark/serializers.py\", line 209, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1504379514127_0006/container_1504379514127_0006_01_000005/pyspark.zip/pyspark/worker.py\", line 71, in <lambda>\n    return lambda *a: f(*a)\n  File \"<ipython-input-11-ca9fdfbab274>\", line 2, in <lambda>\nAttributeError: 'NoneType' object has no attribute 'year'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:156)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:152)\n\tat org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:40)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:395)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:315)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:258)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:256)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1375)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:261)\n\t... 8 more\n"
     ]
    }
   ],
   "source": [
    "# Save CSV back to S3\n",
    "df_clean.write.parquet('s3n://2017edmfasatb/nypd_complaints/data/df_clean.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
