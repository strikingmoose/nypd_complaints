{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration (Part VII - Lat Long Visualization)\n",
    "## Review\n",
    "In the last 2 posts, we reviewed (largely using Spark and Spark SQL (very handy)) all of the interesting fields. All of them except latitude and longitude. I ended the last post puzzled about how to actually plot this many points (5 million points!). Spark didn't have anything to do this, so I had to look elsewhere. The problem here is my whole exercise is to leverage _**distributed computing**_, but at this point, my definition of distributed computing has been... exclusively _**Spark**_. So I lied a bit, but that's only because I'm stupid, lazy, and _**don't**_ want to set up some distributed graphing software and manually configure a cluster across machines.\n",
    "\n",
    "So, if _**Spark**_ won't work... what can we do? After a few google searches, I've come across a library called _**[datashader](https://github.com/bokeh/datashader)**_. Datashader claims to be able to make plots such as:\n",
    "\n",
    "<img src=\"https://github.com/bokeh/datashader/raw/master/docs/images/nyc_races.jpg\" width=\"700\">\n",
    "\n",
    "Lo and behold... that's pretty much what I'm looking for... A map of NYC! In the map above, the folks at datashader have mapped out the most prominent races according to the NYC census data. I feel like that's pretty much what I want, except the most prominent race becomes the prominent type of crime, or by the most prominent seriousness of a crime (violation, misdemeanor, felony).\n",
    "\n",
    "Man... now I feel like a super basic data geek who thinks they're cutting edge because they're working with NYC data...\n",
    "\n",
    "<img src=\"https://booktrib.com/wp-content/uploads/2015/10/basic-bitch.jpg\" width=\"400\">\n",
    "\n",
    "The worst part? It took me 12 posts to realize it. Oh well, onward we move.\n",
    "\n",
    "## Datashader\n",
    "Here's a tutorial of datashader that honestly outputs some breathtaking visualizations.\n",
    "\n",
    "[![IMAGE ALT TEXT](http://img.youtube.com/vi/fB3cUrwxMVY/0.jpg)](http://www.youtube.com/watch?v=fB3cUrwxMVY \"Video Title\")\n",
    "\n",
    "Watching this video actually gives an interesting perspective on big data plotting as well. They approached datashader with a big data motivation, but not in the compute sense... more in the art of visualization sense. Their reasoning is that, simply plotting like a million points in a small area (a common example they use is thinking of how many data points is packed into each pixel on screen) will yield perception and accuracy problems.\n",
    "\n",
    "[This post by the folks at Continuum](https://anaconda.org/jbednar/plotting_pitfalls/notebook?version=2016.11.11.1140) (who created datashader) shows some of the pitfalls of plotting a ton of data. Their argument is that, if any of these 6 pitfalls occur, the visualization may _**literally be lying to you**_.\n",
    "\n",
    "The first one they talk about is _**overplotting**_. This is something I run into all the time and have used the alpha / opacity plotting parameter to solve in the past. Overplotting is when we have 2 classes identified by different colors plotted on the same chart. Let's say one group is plotted with blue, and the other is plotting in red:\n",
    "\n",
    "![](https://s3.ca-central-1.amazonaws.com/2017edmfasatb/nypd_complaints/images/29_overplotting.png)\n",
    "\n",
    "We can see, depending on the order in which we plot the points actually matter. If we plot red first, then blue, we get image _**C**_ which masks many of the red points... we simply don't know if the red is there because we cannot see the bottom layer of the plot! Image _**D**_ is just the opposite - we can infer that there are blue points, and we may have already been biased looking at these sets of photos because we already know the true state of the data, but if we were looking at D for the first time we may not have been able to tell if the blue dots really exist!\n",
    "\n",
    "This is when I'd play around with the alpha, or opacity of the dots to make the visualization a bit more truthful:\n",
    "\n",
    "![](https://s3.ca-central-1.amazonaws.com/2017edmfasatb/nypd_complaints/images/30_oversaturation.png)\n",
    "\n",
    "Even here, we see that it's not perfect as there still is a difference between C and D. The datashader documentation then goes on to state that having to tweak the opacity and the dot sizes (see below with same opacity, but smaller dots)\n",
    "\n",
    "![](https://s3.ca-central-1.amazonaws.com/2017edmfasatb/nypd_complaints/images/31_oversaturation_small_dots.png)\n",
    "\n",
    "makes the visualization process much slower and takes away the focus from what we're actually trying to do! It goes on to describe 4 more pitfalls that could occur when visualization the data, and continues to make the point that\n",
    "\n",
    "> _**For big data, you don't know when the viz is lying**_\n",
    "\n",
    "This comes from datashader's definition of big data that is\n",
    "\n",
    "> _**Data is big when you can't make any inferences from any single point of data**_\n",
    "\n",
    "This implies that the data is so granular (a single incident at a single lat long) that you're not meant to be able to infer any overall conclusions from a single data point, and going through every single point is an inefficient (and in all likelihood impossible) task because of pure volume. In a case like this, _**all you can rely on is the visualization**_, and if the visualization is incorrect, you don't have any QA tools to notice, so it's best to use the right theory from the get go.\n",
    "\n",
    "When I tried to build a heatmap from the [Edmonton Property Assessment data](https://strikingmoose.com/2017/07/31/building-a-regression-model-k-nn/), the method I tried was to build a regression model that predicts an assessment value for each lat long value / range. In K-NN, we can infer lat / longs which do not have a property assessment tied to them by taking a look at those around them. In essence, we build a gradient across the lats and longs to communicate the predicted assessment value. After doing that, my visualization consisted of a set of predictions on _**a grid of inputs across edmonton**_ to ensure I have every area covered.\n",
    "\n",
    "In this example, what I'd like to visualize is the _**volume of incidents**_, which I have inherently in the data, so I don't have to do anything fancy with predictive models. However, I still face all the problems that datashader has laid out. What datashader does is similar to what I had done in the [Edmonton Property Assessment](https://strikingmoose.com/2017/07/31/building-a-regression-model-k-nn/) post in that\n",
    "1. It represents each pixel of the screen is represented by a value, not solely represented by the data points contributing to that pixel\n",
    "2. \\#1 is true because the value is represented by an _**overall model**_ that has the context of _**the entire dataset**_, not just the pixel in question\n",
    "\n",
    "In datashader's example, if we set an alpha to 0.1, we are indicating that 10 points lying on top of each other will achieve _**full saturation**_. If, in our data, we only have points that are either not overlapping (singular points), or they overlap by 50 points at a time, it would be very difficult to create a plot that can show off this contrast well because anything over 10 points at a time will achieve full saturation! Anything between 10 and 50 points overlapping will look the same to the user. That's what happens when we don't generalize our data to some type of heatmap or model. This is where datashader's real advantage comes in, because datashader allows you to easily map your own model without having to go through the trouble that I went through in the [Edmonton Property Assessment](https://strikingmoose.com/2017/07/31/building-a-regression-model-k-nn/) project. Granted, I don't know if datashader can perform regression, but it's got a lot of great gradients (e.g. using log instead of linear) for our specific purpose, analying volumes of points in a region where each point is weighted the same!\n",
    "\n",
    "## But Wait...\n",
    "But wait... what happened to the whole thing about not being able to use Spark anymore? I thought we were trying to leverage distributed computing for this project? I agree, I am being a bit lazy here in dropping distributed computing altogether in this plot, but in the datashader video above, the guy plots _**ONE BILLION POINTS**_, and he does it on his Mac which I assume has no more than 32GB RAM (I'm probably wrong). If I think about it objectively, the inputs of a map are simply latitude and longitudes. The dataset is 1.3 GB, but the lats and longs probably only account for like 1/50th of the entire file size! Lat / longs are bounded by however many significant digits of the lat / long itself. A latitude like 53.631611 will always only take 9 bytes to represent, whereas a text description field is often much longer and variable. Given that there's about 35 rows of data in our parquet dataframe, lat / longs accounting for 1/50th of the file size isn't so farfetched to me! That brings down the file size to 30 MB. Let's say our own laptop had 32 GB memory, we could handle over _**1 TB**_ of just latitude and longitude data in memory.\n",
    "\n",
    "There are different solutions for everything, and perhaps this one is the path of least resistance for mapping functionality!\n",
    "\n",
    "## Back To Datashader\n",
    "Let's try to import the dataset into Jupyter. I've found this package pyarrow which can apparently read parquet files into a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.system(\"sudo pip install pyarrow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it looks like there's a pretty nice python package _**[pyarrow](https://arrow.apache.org/docs/python/parquet.html)**_ which seems to be able to load up parquets in a quick and simple manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import pyarrow\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowIOError",
     "evalue": "Failed to open local file: s3n://2017edmfasatb/nypd_complaints/data/df_filtered.parquet",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArrowIOError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-44807fc45e46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtable2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"s3n://2017edmfasatb/nypd_complaints/data/df_filtered.parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib64/python2.7/site-packages/pyarrow/parquet.pyc\u001b[0m in \u001b[0;36mread_table\u001b[0;34m(source, columns, nthreads, metadata, use_pandas_metadata)\u001b[0m\n\u001b[1;32m    722\u001b[0m                                    metadata=metadata)\n\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 724\u001b[0;31m     \u001b[0mpf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParquetFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    725\u001b[0m     return pf.read(columns=columns, nthreads=nthreads,\n\u001b[1;32m    726\u001b[0m                    use_pandas_metadata=use_pandas_metadata)\n",
      "\u001b[0;32m/usr/local/lib64/python2.7/site-packages/pyarrow/parquet.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, source, metadata, common_metadata)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommon_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParquetReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon_metadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcommon_metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python2.7/site-packages/pyarrow/_parquet.pyx\u001b[0m in \u001b[0;36mpyarrow._parquet.ParquetReader.open (/arrow/python/build/temp.linux-x86_64-2.7/_parquet.cxx:8115)\u001b[0;34m()\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m         \u001b[0mget_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m&\u001b[0m\u001b[0mrd_handle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mnogil\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             check_status(OpenFile(rd_handle, self.allocator, properties,\n",
      "\u001b[0;32mpyarrow/io.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.get_reader (/arrow/python/build/temp.linux-x86_64-2.7/lib.cxx:46747)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpyarrow/io.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.memory_map (/arrow/python/build/temp.linux-x86_64-2.7/lib.cxx:43943)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpyarrow/io.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.MemoryMappedFile._open (/arrow/python/build/temp.linux-x86_64-2.7/lib.cxx:43722)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpyarrow/error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.check_status (/arrow/python/build/temp.linux-x86_64-2.7/lib.cxx:7495)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mArrowIOError\u001b[0m: Failed to open local file: s3n://2017edmfasatb/nypd_complaints/data/df_filtered.parquet"
     ]
    }
   ],
   "source": [
    "table2 = pq.read_table(\"s3n://2017edmfasatb/nypd_complaints/data/df_filtered.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, apparently it's not as straight forward to read a parquet file into a Pandas dataframe as I thought... It looks like, at the time of writing this, [pyarrow does not support reading from partitioned S3...](https://stackoverflow.com/questions/45082832/how-to-read-partitioned-parquet-files-from-s3-using-pyarrow-in-python):\n",
    "\n",
    "I've used the same path string as when I was using Spark in the last post, but I guess Spark, in this case, was spun up from an Amazon EMR cluster which had partitioned S3 integration built in. Pyarrow doesn't seem to have this (yet?). I guess that's why we pay Amazon an additional hourly cost for the EMR service. Obviously not only because of this ability to read parquets, but this capability among other features (like setting up the entire goddamn cluster for us).\n",
    "\n",
    "The stackoverflow post above suggests another library which can read parquets in this moment in time from S3 (at least the guy has got it working). Let's give that a shot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.system(\"sudo pip install fastparquet s3fs python-snappy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import s3fs\n",
    "import fastparquet as fp\n",
    "s3 = s3fs.S3FileSystem()\n",
    "fs = s3fs.core.S3FileSystem()\n",
    "\n",
    "# Set up s3fs path object\n",
    "s3_path = \"2017edmfasatb/nypd_complaints/data/df_filtered.parquet\"\n",
    "all_paths_from_s3 = fs.glob(path = s3_path)\n",
    "\n",
    "# Load file from S3\n",
    "myopen = s3.open\n",
    "#use s3fs as the filesystem\n",
    "fp_obj = fp.ParquetFile(all_paths_from_s3, open_with = myopen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, I just can't get this parquet file to load... What a lesson in data management. I was working so fluidly in my analysis and, SURPRISE, more technical issues. Well, I shouldn't say more technical issues... I should say my own lack of knowledge lol.\n",
    "\n",
    "<img src=\"https://i.giphy.com/media/ncezxIloIjDNu/giphy.webp\" width=\"500\">\n",
    "\n",
    "So, what I _**think**_ is happening here is that Spark seems to be loading and saving parquets in a _**partitioned manner**_. Again, this is what my parquet file looks like on EMRFS (which is then abstracted on top of the actual file systems in the underlying clusters):\n",
    "\n",
    "<img src=\"https://s3.ca-central-1.amazonaws.com/2017edmfasatb/nypd_complaints/images/28_parquet_aws.png\" width=\"600\">\n",
    "\n",
    "First of all, the _**.parquet \"file\"**_ is actually a _**folder**_, and the above image is the partitioned EMRFS pieces within that .parquet folder. These files are also .parquet files as well, actually. Snappy seems to be a method of compression. I might just have to go back to Spark and actually use Spark to load up this dataframe, then convert it to Pandas to be used with datashader. That just seems too convoluted though, there must be an easier way.\n",
    "\n",
    "_**-- 5 minutes later --**_\n",
    "\n",
    "Wow, I'm an idiot.\n",
    "\n",
    "<img src=\"https://i.giphy.com/media/ncezxIloIjDNu/giphy.webp\" width=\"500\">\n",
    "\n",
    "The answer was right there in the [stackoverflow answer](https://stackoverflow.com/questions/45082832/how-to-read-partitioned-parquet-files-from-s3-using-pyarrow-in-python)\n",
    "\n",
    "In the answer, the user writes\n",
    "\n",
    "~~~~\n",
    "#mybucket/data_folder/serial_number=1/cur_date=20-12-2012/abcdsd0324324.snappy.parquet \n",
    "s3_path = \"mybucket/data_folder/*/*/*.parquet\"\n",
    "~~~~\n",
    "\n",
    "I actually changed the path to my top level .parquet file, which, again, _**was my .parquet folder**_, when I really needed to point the path to the _**multiple, individual, .snappy.parquet files**_ within that folder. There was a lot of new concepts to me here... Spark, partitioned storage, parquet... I'm glad it's somewhat coming together now. Let's try this one more time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-73d536981674>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Set up s3fs path object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0ms3_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"2017edmfasatb/nypd_complaints/data/df_filtered.parquet/*.parquet\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mall_paths_from_s3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms3_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Load file from S3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fs' is not defined"
     ]
    }
   ],
   "source": [
    "# Set up s3fs path object\n",
    "s3_path = \"2017edmfasatb/nypd_complaints/data/df_filtered.parquet/*.parquet\"\n",
    "all_paths_from_s3 = fs.glob(path = s3_path)\n",
    "\n",
    "# Load file from S3\n",
    "myopen = s3.open\n",
    "#use s3fs as the filesystem\n",
    "fp_obj = fp.ParquetFile(all_paths_from_s3, open_with = myopen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems to have worked.\n",
    "\n",
    "![](https://i.giphy.com/media/xeXEpUVvAxCV2/giphy.webp)\n",
    "\n",
    "Now that I look at the [fastparquet documentation](https://fastparquet.readthedocs.io/en/latest/), they literally say:\n",
    "\n",
    "> _**read and write Parquet files, in single- or multiple-file format. The latter is common found in hive/Spark usage.**_\n",
    "\n",
    "It's literally the first feature they list. In addition, they also allow us to [load specific columns](https://fastparquet.readthedocs.io/en/latest/quickstart.html#reading) (nice feature of parquet I'll finally get to see in action) to a Pandas dataframe! Let's try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fp_obj' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-90449ee3af62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu\"# Load only lat and long to pandas dataframe\\ndf = fp_obj.to_pandas(['LAT', 'LON'])\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python2.7/dist-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/dist-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/dist-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1183\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1185\u001b[0;31m             \u001b[0;32mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1186\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fp_obj' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Load only lat and long to pandas dataframe\n",
    "df = fp_obj.to_pandas(['LAT', 'LON'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 seconds to theoretically load 5M+ latitude and longitudes. Not quite sure how to benchmark that, but it doesn't put a damper in my day as of yet, so I'll take it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-9625c2a91419>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# View dataframe columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# View dataframe columns\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yup, pretty much what I'm looking for! We're so far into this post already (it seems), and I haven't even touched datashader. Finally, we are here, but I'm almost expecting it to take like another 3 posts until I see an actual map like the one above haha.\n",
    "\n",
    "## Datashader... ACTUALLY this time...\n",
    "The first thing we have to do is install datashader because the [Amazon Deep Learning AMI](https://aws.amazon.com/marketplace/pp/B01M0AXXQB) doesn't come with datashader right off the bat. Datashader is not currently hosted on the pypi repo, so we have to install it using conda.\n",
    "\n",
    "Since Anaconda2 and Anaconda3 are installed on the Amazon Deep Learning AMI, we have to specify to use Anaconda2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install datashader via conda, we use the -y flag so we don't have to reply to any prompts\n",
    "import os\n",
    "os.system(\"sudo /home/ec2-user/src/anaconda2/bin/conda install -y bokeh datashader\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'datashader'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-cad1cea0110a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdatashader\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# from datashader import transfer_functions as tf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# from datashader.colors import Greys9\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Greys9_r = list(reversed(Greys9))[:-2]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'datashader'"
     ]
    }
   ],
   "source": [
    "import datashader as ds\n",
    "# from datashader import transfer_functions as tf\n",
    "# from datashader.colors import Greys9\n",
    "# Greys9_r = list(reversed(Greys9))[:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/python3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
