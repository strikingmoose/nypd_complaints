{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering To Explore Neighbourhoods\n",
    "## Review\n",
    "In the last post, we reviewed some of the visualizations we made via datashader and learned a few things about the neighbourhoods in NYC. As someone who is not from NYC, has never lived in NYC, and has only ever visited Manhattan and Brooklyn (and even then, my exposure is quite limited), I was really amazed at how hard of a boundary there were between different neighbourhoods and different types / severity of offenses! I don't think the takeaways are mindblowing as I'm sure someone living in the area is familiar with the general safety of the boroughs, but I came to these conclusions with a MacBook, some chips and guac, and a couch in the comfort of my home here in Edmonton, Alberta... the NY of Canada.\n",
    "\n",
    "![](https://a1.cdn-hotels.com/cos/production183/d888/1c4131d0-86eb-11e6-a741-0242ac110051.jpg)\n",
    "\n",
    "The resemblance is _**uncanny**_.\n",
    "\n",
    "Point being I'm learning via data, and data alone. I'm not saying that looking at this data is a substitute for being there either - By not being there, I have to be comfortable with the fact that I have no supporting information _**outside**_ of the data. This dataset only has a handful of columns, but surely there is information which is not feasible for the NYPD to collect. In fact, there's probably data that the NYPD has not even made open to the public! If we think about something like Grand Larceny, there can be so many other factors that may describe the crime and tell a story within itself... the square footage of the unit, the value of goods stolen, the ethnicity / race of the criminal / victim... there are pretty much endless data points we can capture if we want to get really technical and creative, but obviously there is a balance between how much data you _**can**_ collect vs how much data you _**do**_ collect.\n",
    "\n",
    "Regardless, it's pretty cool to be learning through data, and data alone. Reminds me of when I tried to [predict the All-NBA players](https://strikingmoose.com/2017/07/31/predicting-2016-2017-all-nba-teams-end-to-end/). If I knew how to run a gradient boosted tree, I basically didn't have to know _**anything**_ about basketball. A scary though, and obviously not always the right train of thought, but always crazy to see where data, alone, can pave the way.\n",
    "\n",
    "## Contextualizing Visualizations\n",
    "I want to take a few minutes to write about the use of visualiations, and why the title of this post is on clustering.\n",
    "\n",
    "The datashader visualizations were beautiful and really informative, but visualizations themselves have pitfalls as well - We gain simplicity and clarity by eliminating detail. I mean, isn't that the whole point of a visualization anyways? Why use a visualization if the data was simple? We make visualizations in the first place because our dataset is too complicated to look at in a tabular form! The inner workings of datashader is based on this principal. Instead of looking at each point individually, datashader aggregates by a more feasible approach for a computer screen... the pixel! Each pixel is summarized based on the data points that fall into that pixel's boundary, and the data is summarized that way.\n",
    "\n",
    "In datashader, we use the **eq\\_hist** color method which tries to let each shade of color in our color gradient / map represent an equal _**amount of samples**_ in our entire dataset. When we apply this coloring method, we lose a realistic handle on the magnitude of our data. eq\\_hist is literally _**manipulating the values of the data to appease our eyes**_.\n",
    "\n",
    "When we categorize the data by the offense level or offense type, we are only looking at 3-4 categories at a time. What about all the other categories? For now, we've just left them out of the visualization altogether. Is this the right thing to do? Well, it really depends on the question we're trying to answer, doesn't it? I realize I came into this entire exercise without any real objectives other than to just learn a bit more about crime in NYC, but when I was using datashader, I was inherently doing a geographical analysis. The map we generated below\n",
    "\n",
    "<img src=\"https://s3.ca-central-1.amazonaws.com/2017edmfasatb/nypd_complaints/images/35_nypd_complaints_offense_type_edited.png\" width=\"500\">\n",
    "\n",
    "showed 3 / 10 possible values we had for the OFFENSE\\_DESCRIPTION column (which, itself, was grouped down from 50+ categories). By ignoring the data, we are making the same conscious decision that we made when we decided to visualize in the first place. We want to look at the data through a _**different lens**_ in order to get a different perspective. Whether that perspective is the right one to take to answer that specific question... that's where the art of it comes into play. When I created that graph, I had a general sense that wealthier neighbourhoods experienced more Grand Larceny and lesser well-off neighbourhoods experienced more Dangerous Drug offenses while Harrassment was widespread throughout all of NYC. That visualization worked out perfectly for me because the boundaries were so clearly defined across the 3 offense types. The reality of the situation is that I feel uneasy plotting anything over 3-4 categories per map. Can you imagine a map with 10 categories, let alone 50? Can we even process all the colors? Can we even fit a legend on the plot? Visualization inherently has the ability to only summarize a limited amount of information due to our own limitations as humans.\n",
    "\n",
    "## Clustering\n",
    "I'm now feeling a bit uneasy because of I've ignored so many types of offenses... But wait, didn't I ignore them in the first place so I could make my visualization simpler to understand? Man... this is confusing...\n",
    "\n",
    "<img src=\"https://i.giphy.com/media/l2R01mSIsazqNQ7ks/giphy.webp\" width=\"400\">\n",
    "\n",
    "You know, it just comes down the fact that I'm not coming into this project with any specific objective. I'm looking for some type of finding, but I don't know which 8 questions I need to ask to get there. I'm digging for gold, but I haven't a clue where to look or what tools I need... hell... I don't even know if it's _**gold**_ I'm looking for! That's what this blog is for right...? To have a sandbox where nobody judges me and I can fail as much as I want!\n",
    "\n",
    "<img src=\"https://media.collegetimes.com/uploads/2016/02/09152603/laughing-then-crying.gif\" width=\"500\">\n",
    "\n",
    "Because I'm feeling so sensitive to those offenses that I ignored, I want to find a middle ground between looking at the tabular data and and looking at a limited number of categories on a map.\n",
    "\n",
    "In comes clustering.\n",
    "\n",
    "When I was checking out one of the [Kaggle NYC Taxi competition kernels](https://www.kaggle.com/gaborfodor/from-eda-to-the-top-lb-0-367), I noticed a super interesting use of clustering. Yes, yes, blah, blah I'm naive, I get it, but it still was really cool for me to see. About a third of a way down the notebook, the kaggler uses K-means clustering to actually cluster all the taxi data into _**regions**_, and these regions loosely represent a small neighbourhood in NYC. Ideally, we would have the boundaries of each neighbourhood defined in code so we can explicitly define each neighbourhood rather than _**approximating**_ their definition, but in the case of taxis, the hard boundaries may not matter anyways because taxis run in and out of neighbourhoods continuously. Oh yea, also we don't have the boundaries in code, so... yeah. This kaggler uses clustering to see if the neighbourhood the taxis are leaving from has a huge impact on the trip duration.\n",
    "\n",
    "In my case, I could also use clustering to define my data into 20 or so clusters. If I approximate NYC as 20 clusters of neighbourhoods, I might be able to better _**view**_ the data. Again, we're losing detail here as we're summarizing 5 million points into 20 clusters, but it will give us more detail than the map did as we'll be able to just look at the offense statistics / averages for a manageable amount of data samples (one row of data for each cluster).\n",
    "\n",
    "## Spark ML\n",
    "Because we are working with the full dataset again, we need to (or want to because our dataset isn't that big anyways) bring Spark back. Another reason for Spark's dominance is that it comes with a _**distributed, out of the box, easy to use machine learning library, Spark ML**_. Spark ML has a K-means clustering algorithm built in - lucky for us!\n",
    "\n",
    "#### Initial Setup & Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.system(\"sudo pip install findspark sql_magic pyspark_dist_explore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    require(['notebook/js/codecell'], function(codecell) {\n",
       "      // https://github.com/jupyter/notebook/issues/2453\n",
       "      codecell.CodeCell.options_default.highlight_modes['magic_text/x-sql'] = {'reg':[/^%read_sql/, /.*=\\s*%read_sql/,\n",
       "                                                                                      /^%%read_sql/]};\n",
       "      Jupyter.notebook.events.one('kernel_ready.Kernel', function(){\n",
       "          console.log('BBBBB');\n",
       "          Jupyter.notebook.get_cells().map(function(cell){\n",
       "              if (cell.cell_type == 'code'){ cell.auto_highlight(); } }) ;\n",
       "      });\n",
       "    });\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use findspark package to connect Jupyter to Spark shell\n",
    "import findspark\n",
    "findspark.init('/usr/lib/spark')\n",
    "\n",
    "# Load SparkSession object\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Load other libraries\n",
    "from datetime import datetime\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DateType\n",
    "from functools import reduce\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Graphing with matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "\n",
    "# Initiate SparkSession as \"spark\"\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load sql_magic and connect to Spark\n",
    "%load_ext sql_magic\n",
    "%config SQL.conn_name = 'spark'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 14.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Read NYPD Complaint Data\n",
    "df_filtered = spark.read.parquet(\"s3n://2017edmfasatb/nypd_complaints/data/df_filtered.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5336177"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- COMPLAINT_NUMBER: integer (nullable = true)\n",
      " |-- COMPLAINT_START_TIMESTAMP: timestamp (nullable = true)\n",
      " |-- COMPLAINT_END_TIMESTAMP: timestamp (nullable = true)\n",
      " |-- REPORTED_DATE_TIMESTAMP: timestamp (nullable = true)\n",
      " |-- COMPLAINT_START_TIMESTAMP_YEAR: string (nullable = true)\n",
      " |-- COMPLAINT_START_TIMESTAMP_MONTH: string (nullable = true)\n",
      " |-- COMPLAINT_START_TIMESTAMP_DAY: string (nullable = true)\n",
      " |-- COMPLAINT_START_TIMESTAMP_WEEKDAY: string (nullable = true)\n",
      " |-- COMPLAINT_START_TIMESTAMP_HOUR: string (nullable = true)\n",
      " |-- COMPLAINT_END_TIMESTAMP_YEAR: string (nullable = true)\n",
      " |-- COMPLAINT_END_TIMESTAMP_MONTH: string (nullable = true)\n",
      " |-- COMPLAINT_END_TIMESTAMP_DAY: string (nullable = true)\n",
      " |-- COMPLAINT_END_TIMESTAMP_WEEKDAY: string (nullable = true)\n",
      " |-- COMPLAINT_END_TIMESTAMP_HOUR: string (nullable = true)\n",
      " |-- REPORTED_DATE_TIMESTAMP_YEAR: string (nullable = true)\n",
      " |-- REPORTED_DATE_TIMESTAMP_MONTH: string (nullable = true)\n",
      " |-- REPORTED_DATE_TIMESTAMP_DAY: string (nullable = true)\n",
      " |-- REPORTED_DATE_TIMESTAMP_WEEKDAY: string (nullable = true)\n",
      " |-- COMPLAINT_LENGTH: double (nullable = true)\n",
      " |-- COMPLAINT_LENGTH_ZERO_TIME: boolean (nullable = true)\n",
      " |-- COMPLAINT_START_REPORTED_LAG: double (nullable = true)\n",
      " |-- COMPLAINT_END_REPORTED_LAG: double (nullable = true)\n",
      " |-- OFFENSE_DESCRIPTION: string (nullable = true)\n",
      " |-- OFFENSE_RESULT: string (nullable = true)\n",
      " |-- OFFENSE_LEVEL: string (nullable = true)\n",
      " |-- JURISDICTION: string (nullable = true)\n",
      " |-- BOROUGH: string (nullable = true)\n",
      " |-- PRECINCT: integer (nullable = true)\n",
      " |-- SPECIFIC_LOCATION: string (nullable = true)\n",
      " |-- PREMISE_DESCRIPTION: string (nullable = true)\n",
      " |-- LAT: double (nullable = true)\n",
      " |-- LON: double (nullable = true)\n",
      " |-- COMPLAINT_LENGTH_DAYS: double (nullable = true)\n",
      " |-- COMPLAINT_LENGTH_UNDER_ONE_YEAR: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_filtered.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering\n",
    "\n",
    "![](https://www.projectrhea.org/rhea/images/e/ef/RunyanKmeans.gif)\n",
    "\n",
    "Why not start with a gif that basically explains everything. I'm not even sure what else I need to say haha. K-Means clustering is an iterative method which finds natural clusters of data samples.\n",
    "\n",
    "To start off, we have to tell K-Means _**how many clusters**_ we want. K-Means, unfortunately, doesn't tell this us explicitly (although we can use within-cluster error metrics to help us tune this). The traditional use case of clustering is a bit different from what we're using it for in this project. Traditionally, maybe you work on a product team and you're trying to figure out which users to offer discounts to. Maybe you have 3 tiers of discounts you'd like to give, and you'd like K-Means to find these 3 groups for us naturally based on historical data. We'd go into K-Means saying we want 3 explicit groups and let the algorithm to the rest.\n",
    "\n",
    "In this project, we're kinda using clustering to represent general regions across NYC. Do we care if we represent NYC as 20 or 21 clusters? Maybe 22? Why don't way go crazy and say 23? It really doesn't matter because I'm just building general clusters to break out the city into some arbitrary number that will help me view some summary statistics by geography. I'm not explicitly saying \"New York can only have 20 neighbourhoods, and that's final!\".\n",
    "\n",
    "<img src=\"http://cdn4.gurl.com/wp-content/uploads/2014/03/scandal-disagree.gif\" width=\"400\">\n",
    "\n",
    "K-Means iterates between two steps:\n",
    "- Updating cluster assignments\n",
    "- Updating cluster centres\n",
    "\n",
    "Once we tell K-Means how many clusters we're looking for, K-Means _**randomly assigns cluster centres**_ in the dimensional space that we've given it. The hope, through iteratively finding new cluster centres, is that the cluster centres and assignments will eventually represent natural spatial clusters in the data.\n",
    "\n",
    "In the clustering gif above, we see what most would probably perceive as 3 clusters in the data:\n",
    "- Top left\n",
    "- Top right\n",
    "- Bottom right\n",
    "\n",
    "Originally, the cluster centres are in the bottom left corner where there is no cluster, and every point is classified under the blue cluster. However, after updating the cluster centre the first time, we see some red starting to emerge. Soon enough, the cluster centres have auto-magically found the natural clusters that our eyes saw. WOW!\n",
    "\n",
    "<img src=\"https://media.tenor.com/images/136313988be72de056952b46f455868e/tenor.gif\" width=\"250\">\n",
    "\n",
    "The magic behind the scenes is basically a single cluster mean calculation every iteration. Yeah... I don't really have anything else to add to that lol.\n",
    "\n",
    "The last thing I will say is that, sometimes, the structure of the data will make it such that different initilization locations of the cluster centres yield different cluster assignments upon convergence. For our example, it shouldn't matter here as our clusters ar a bit arbitrary to begin with.\n",
    "\n",
    "Let's try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "'Field \"features\" does not exist.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o53.fit.\n: java.lang.IllegalArgumentException: Field \"features\" does not exist.\n\tat org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:266)\n\tat org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:266)\n\tat scala.collection.MapLike$class.getOrElse(MapLike.scala:128)\n\tat scala.collection.AbstractMap.getOrElse(Map.scala:59)\n\tat org.apache.spark.sql.types.StructType.apply(StructType.scala:265)\n\tat org.apache.spark.ml.util.SchemaUtils$.checkColumnType(SchemaUtils.scala:40)\n\tat org.apache.spark.ml.clustering.KMeansParams$class.validateAndTransformSchema(KMeans.scala:93)\n\tat org.apache.spark.ml.clustering.KMeans.validateAndTransformSchema(KMeans.scala:254)\n\tat org.apache.spark.ml.clustering.KMeans.transformSchema(KMeans.scala:340)\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:74)\n\tat org.apache.spark.ml.clustering.KMeans.fit(KMeans.scala:305)\n\tat org.apache.spark.ml.clustering.KMeans.fit(KMeans.scala:254)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d118b0a0041f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Set seed for ability to reproduce results, 20 clusters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mkmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_filtered\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'LAT'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'LON'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \"\"\"\n\u001b[1;32m    261\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: 'Field \"features\" does not exist.'"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "# Set seed for ability to reproduce results, 20 clusters\n",
    "kmeans = KMeans(k = 20, seed = 1)\n",
    "model = kmeans.fit(df_filtered[['LAT', 'LON']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, it looks like we need to [transform the dataframe into vectors first...](https://stackoverflow.com/questions/40838893/pyspark-using-dataframe-in-ml-algorithms) This stackoverflow answer actually links to some official Spark documentation [giving an example of this transformation as well](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.clustering.KMeans)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Initiate and transform columns into vector\n",
    "vecAssembler = VectorAssembler(inputCols = ['LAT', 'LON'], outputCol = \"features\")\n",
    "k_means_input = vecAssembler.transform(df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24 ms, sys: 0 ns, total: 24 ms\n",
      "Wall time: 1min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Refit model\n",
    "model = kmeans.fit(k_means_input[['features']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Took about a minute to create the model. Let's assign the clusters back to the data as a separate column of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(features=DenseVector([40.8288, -73.9167]), prediction=0), Row(features=DenseVector([40.6973, -73.7846]), prediction=4), Row(features=DenseVector([40.8026, -73.9451]), prediction=16), Row(features=DenseVector([40.6545, -73.7263]), prediction=12), Row(features=DenseVector([40.738, -73.9879]), prediction=17)]\n",
      "CPU times: user 12 ms, sys: 0 ns, total: 12 ms\n",
      "Wall time: 1.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Use model to assign the samples a cluster to belong to\n",
    "prediction = model.transform(k_means_input[['features']])\n",
    "print(prediction.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:37511)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:37511)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.4/dist-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-16-98529eb2d930>\", line 2, in <module>\n",
      "    df_filtered_indexed = df_filtered.withColumn('row_index', F.monotonically_increasing_id())\n",
      "  File \"/usr/lib/spark/python/pyspark/sql/functions.py\", line 452, in monotonically_increasing_id\n",
      "    return Column(sc._jvm.functions.monotonically_increasing_id())\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 1525, in __getattr__\n",
      "    \"\\n\" + proto.END_COMMAND_PART)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 881, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 829, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 835, in _create_connection\n",
      "    connection.start()\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 970, in start\n",
      "    raise Py4JNetworkError(msg, e)\n",
      "py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:37511)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.4/dist-packages/IPython/core/interactiveshell.py\", line 1806, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JNetworkError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:37511)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.4/dist-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-16-98529eb2d930>\", line 2, in <module>\n",
      "    df_filtered_indexed = df_filtered.withColumn('row_index', F.monotonically_increasing_id())\n",
      "  File \"/usr/lib/spark/python/pyspark/sql/functions.py\", line 452, in monotonically_increasing_id\n",
      "    return Column(sc._jvm.functions.monotonically_increasing_id())\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 1525, in __getattr__\n",
      "    \"\\n\" + proto.END_COMMAND_PART)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 881, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 829, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 835, in _create_connection\n",
      "    connection.start()\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 970, in start\n",
      "    raise Py4JNetworkError(msg, e)\n",
      "py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:37511)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.4/dist-packages/IPython/core/interactiveshell.py\", line 1806, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JNetworkError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:37511)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.4/dist-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-16-98529eb2d930>\", line 2, in <module>\n",
      "    df_filtered_indexed = df_filtered.withColumn('row_index', F.monotonically_increasing_id())\n",
      "  File \"/usr/lib/spark/python/pyspark/sql/functions.py\", line 452, in monotonically_increasing_id\n",
      "    return Column(sc._jvm.functions.monotonically_increasing_id())\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 1525, in __getattr__\n",
      "    \"\\n\" + proto.END_COMMAND_PART)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 881, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 829, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 835, in _create_connection\n",
      "    connection.start()\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 970, in start\n",
      "    raise Py4JNetworkError(msg, e)\n",
      "py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:37511)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.4/dist-packages/IPython/core/interactiveshell.py\", line 1806, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JNetworkError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:37511)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.4/dist-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-16-98529eb2d930>\", line 2, in <module>\n",
      "    df_filtered_indexed = df_filtered.withColumn('row_index', F.monotonically_increasing_id())\n",
      "  File \"/usr/lib/spark/python/pyspark/sql/functions.py\", line 452, in monotonically_increasing_id\n",
      "    return Column(sc._jvm.functions.monotonically_increasing_id())\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 1525, in __getattr__\n",
      "    \"\\n\" + proto.END_COMMAND_PART)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 881, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 829, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 835, in _create_connection\n",
      "    connection.start()\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 970, in start\n",
      "    raise Py4JNetworkError(msg, e)\n",
      "py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:37511)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.4/dist-packages/IPython/core/interactiveshell.py\", line 1806, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JNetworkError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:37511)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.4/dist-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-16-98529eb2d930>\", line 2, in <module>\n",
      "    df_filtered_indexed = df_filtered.withColumn('row_index', F.monotonically_increasing_id())\n",
      "  File \"/usr/lib/spark/python/pyspark/sql/functions.py\", line 452, in monotonically_increasing_id\n",
      "    return Column(sc._jvm.functions.monotonically_increasing_id())\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 1525, in __getattr__\n",
      "    \"\\n\" + proto.END_COMMAND_PART)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 881, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 829, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 835, in _create_connection\n",
      "    connection.start()\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 970, in start\n",
      "    raise Py4JNetworkError(msg, e)\n",
      "py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:37511)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.4/dist-packages/IPython/core/interactiveshell.py\", line 1806, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JNetworkError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:37511)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.4/dist-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-16-98529eb2d930>\", line 2, in <module>\n",
      "    df_filtered_indexed = df_filtered.withColumn('row_index', F.monotonically_increasing_id())\n",
      "  File \"/usr/lib/spark/python/pyspark/sql/functions.py\", line 452, in monotonically_increasing_id\n",
      "    return Column(sc._jvm.functions.monotonically_increasing_id())\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 1525, in __getattr__\n",
      "    \"\\n\" + proto.END_COMMAND_PART)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 881, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 829, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 835, in _create_connection\n",
      "    connection.start()\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 970, in start\n",
      "    raise Py4JNetworkError(msg, e)\n",
      "py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:37511)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.4/dist-packages/IPython/core/interactiveshell.py\", line 1806, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JNetworkError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:37511)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.4/dist-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-16-98529eb2d930>\", line 2, in <module>\n",
      "    df_filtered_indexed = df_filtered.withColumn('row_index', F.monotonically_increasing_id())\n",
      "  File \"/usr/lib/spark/python/pyspark/sql/functions.py\", line 452, in monotonically_increasing_id\n",
      "    return Column(sc._jvm.functions.monotonically_increasing_id())\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 1525, in __getattr__\n",
      "    \"\\n\" + proto.END_COMMAND_PART)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 881, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 829, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 835, in _create_connection\n",
      "    connection.start()\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 970, in start\n",
      "    raise Py4JNetworkError(msg, e)\n",
      "py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:37511)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.4/dist-packages/IPython/core/interactiveshell.py\", line 1806, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JNetworkError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:37511)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.4/dist-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-16-98529eb2d930>\", line 2, in <module>\n",
      "    df_filtered_indexed = df_filtered.withColumn('row_index', F.monotonically_increasing_id())\n",
      "  File \"/usr/lib/spark/python/pyspark/sql/functions.py\", line 452, in monotonically_increasing_id\n",
      "    return Column(sc._jvm.functions.monotonically_increasing_id())\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 1525, in __getattr__\n",
      "    \"\\n\" + proto.END_COMMAND_PART)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 881, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 829, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 835, in _create_connection\n",
      "    connection.start()\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 970, in start\n",
      "    raise Py4JNetworkError(msg, e)\n",
      "py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:37511)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.4/dist-packages/IPython/core/interactiveshell.py\", line 1806, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JNetworkError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:37511)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.4/dist-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-16-98529eb2d930>\", line 2, in <module>\n",
      "    df_filtered_indexed = df_filtered.withColumn('row_index', F.monotonically_increasing_id())\n",
      "  File \"/usr/lib/spark/python/pyspark/sql/functions.py\", line 452, in monotonically_increasing_id\n",
      "    return Column(sc._jvm.functions.monotonically_increasing_id())\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 1525, in __getattr__\n",
      "    \"\\n\" + proto.END_COMMAND_PART)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 881, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 829, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 835, in _create_connection\n",
      "    connection.start()\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 970, in start\n",
      "    raise Py4JNetworkError(msg, e)\n",
      "py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:37511)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.4/dist-packages/IPython/core/interactiveshell.py\", line 1806, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JNetworkError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:37511)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.4/dist-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-16-98529eb2d930>\", line 2, in <module>\n",
      "    df_filtered_indexed = df_filtered.withColumn('row_index', F.monotonically_increasing_id())\n",
      "  File \"/usr/lib/spark/python/pyspark/sql/functions.py\", line 452, in monotonically_increasing_id\n",
      "    return Column(sc._jvm.functions.monotonically_increasing_id())\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 1525, in __getattr__\n",
      "    \"\\n\" + proto.END_COMMAND_PART)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 881, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 829, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 835, in _create_connection\n",
      "    connection.start()\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 970, in start\n",
      "    raise Py4JNetworkError(msg, e)\n",
      "py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:37511)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.4/dist-packages/IPython/core/interactiveshell.py\", line 1806, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JNetworkError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:37511)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.4/dist-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-16-98529eb2d930>\", line 2, in <module>\n",
      "    df_filtered_indexed = df_filtered.withColumn('row_index', F.monotonically_increasing_id())\n",
      "  File \"/usr/lib/spark/python/pyspark/sql/functions.py\", line 452, in monotonically_increasing_id\n",
      "    return Column(sc._jvm.functions.monotonically_increasing_id())\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 1525, in __getattr__\n",
      "    \"\\n\" + proto.END_COMMAND_PART)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 881, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 829, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 835, in _create_connection\n",
      "    connection.start()\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 970, in start\n",
      "    raise Py4JNetworkError(msg, e)\n",
      "py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:37511)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.4/dist-packages/IPython/core/interactiveshell.py\", line 1806, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JNetworkError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "ename": "Py4JNetworkError",
     "evalue": "An error occurred while trying to connect to the Java server (127.0.0.1:37511)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    826\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 827\u001b[0;31m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeque\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    828\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    962\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    964\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_connected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPy4JNetworkError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-98529eb2d930>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# since there are no common column between these two dataframes add row_index so that it can be joined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_filtered_indexed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_filtered\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'row_index'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonically_increasing_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprediction_indexed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'row_index'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonically_increasing_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/functions.py\u001b[0m in \u001b[0;36mmonotonically_increasing_id\u001b[0;34m()\u001b[0m\n\u001b[1;32m    450\u001b[0m     \"\"\"\n\u001b[1;32m    451\u001b[0m     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonically_increasing_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1523\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREFLECTION_COMMAND_NAME\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1524\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREFL_GET_UNKNOWN_SUB_COMMAND_NAME\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1525\u001b[0;31m             \"\\n\" + proto.END_COMMAND_PART)\n\u001b[0m\u001b[1;32m   1526\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSUCCESS_PACKAGE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1527\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mJavaPackage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway_client\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjvm_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    879\u001b[0m          \u001b[0;32mif\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m         \"\"\"\n\u001b[0;32m--> 881\u001b[0;31m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    827\u001b[0m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeque\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    830\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_create_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    833\u001b[0m         connection = GatewayConnection(\n\u001b[1;32m    834\u001b[0m             self.gateway_parameters, self.gateway_property)\n\u001b[0;32m--> 835\u001b[0;31m         \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    968\u001b[0m                 \u001b[0;34m\"server ({0}:{1})\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 970\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mPy4JNetworkError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JNetworkError\u001b[0m: An error occurred while trying to connect to the Java server (127.0.0.1:37511)"
     ]
    }
   ],
   "source": [
    "# since there are no common column between these two dataframes add row_index so that it can be joined\n",
    "df_filtered_indexed = df_filtered.withColumn('row_index', F.monotonically_increasing_id())\n",
    "prediction_indexed = prediction.withColumn('row_index', F.monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 'col3' from second dataframe (i.e. df2) is added to first dataframe (i.e. df1)\n",
    "df_predicted = df_filtered_indexed.join(prediction_indexed, df_filtered_indexed.row_index == prediction_indexed.row_index, 'inner').drop(prediction_indexed.row_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:37511)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 827, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 963, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "ename": "Py4JNetworkError",
     "evalue": "An error occurred while trying to connect to the Java server (127.0.0.1:37511)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    826\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 827\u001b[0;31m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeque\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    828\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    962\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    964\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_connected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPy4JNetworkError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-b8032c6e2bd5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_predicted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mhead\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    970\u001b[0m             \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrs\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    973\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Bob'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m         \"\"\"\n\u001b[0;32m--> 476\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mlimit\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m         \"\"\"\n\u001b[0;32m--> 465\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1129\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1133\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    879\u001b[0m          \u001b[0;32mif\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m         \"\"\"\n\u001b[0;32m--> 881\u001b[0;31m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    827\u001b[0m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeque\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    830\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_create_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    833\u001b[0m         connection = GatewayConnection(\n\u001b[1;32m    834\u001b[0m             self.gateway_parameters, self.gateway_property)\n\u001b[0;32m--> 835\u001b[0;31m         \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    968\u001b[0m                 \u001b[0;34m\"server ({0}:{1})\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 970\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mPy4JNetworkError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JNetworkError\u001b[0m: An error occurred while trying to connect to the Java server (127.0.0.1:37511)"
     ]
    }
   ],
   "source": [
    "df_predicted.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(prediction=0),\n",
       " Row(prediction=4),\n",
       " Row(prediction=16),\n",
       " Row(prediction=12),\n",
       " Row(prediction=17)]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction[['prediction']].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the assigned cluster labels in a column called \"prediction\" now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7ff797b3a908>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQjAzMwD+P9y2IqlpfbWgAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff7c07573c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "\n",
    "# Initiate plot\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(centers['LON'], centers['LAT'])\n",
    "\n",
    "# for i, txt in enumerate(n):\n",
    "#     ax.annotate(txt, (z[i],y[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
