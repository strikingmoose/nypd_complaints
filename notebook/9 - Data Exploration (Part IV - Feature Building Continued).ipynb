{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration (Part III - Feature Building)\n",
    "Let's get right into it. If you've just landed on this post, please read the past few \"Data Exploration\" posts on this project to understand the context of what I'm trying to do here in this post. Note that this post is _**almost**_ identical to the last with the exception of replacing all 24:00:00 time values with 00:00:00."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.system(\"sudo pip install findspark sql_magic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    require(['notebook/js/codecell'], function(codecell) {\n",
       "      // https://github.com/jupyter/notebook/issues/2453\n",
       "      codecell.CodeCell.options_default.highlight_modes['magic_text/x-sql'] = {'reg':[/^%read_sql/, /.*=\\s*%read_sql/,\n",
       "                                                                                      /^%%read_sql/]};\n",
       "      Jupyter.notebook.events.one('kernel_ready.Kernel', function(){\n",
       "          console.log('BBBBB');\n",
       "          Jupyter.notebook.get_cells().map(function(cell){\n",
       "              if (cell.cell_type == 'code'){ cell.auto_highlight(); } }) ;\n",
       "      });\n",
       "    });\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use findspark package to connect Jupyter to Spark shell\n",
    "import findspark\n",
    "findspark.init('/usr/lib/spark')\n",
    "\n",
    "# Load SparkSession object\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Load other libraries\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col, udf, count, isnan, lit, sum, coalesce, concat, to_date, to_timestamp, when, date_format, unix_timestamp, regexp_replace\n",
    "from pyspark.sql.types import DateType\n",
    "from functools import reduce\n",
    "import pandas as pd\n",
    "\n",
    "# Initiate SparkSession as \"spark\"\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load sql_magic and connect to Spark\n",
    "%load_ext sql_magic\n",
    "%config SQL.conn_name = 'spark'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8 ms, sys: 0 ns, total: 8 ms\n",
      "Wall time: 42.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Read NYPD Complaint Data\n",
    "df = spark.read.csv(\n",
    "    \"s3n://2017edmfasatb/nypd_complaints/data/NYPD_Complaint_Data_Historic.csv\", \n",
    "    header = True, \n",
    "    inferSchema = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- COMPLAINT_NUMBER: integer (nullable = true)\n",
      " |-- COMPLAINT_START_DATE: string (nullable = true)\n",
      " |-- COMPLAINT_START_TIME: string (nullable = true)\n",
      " |-- COMPLAINT_END_DATE: string (nullable = true)\n",
      " |-- COMPLAINT_END_TIME: string (nullable = true)\n",
      " |-- REPORTED_DATE: string (nullable = true)\n",
      " |-- OFFENSE_ID: integer (nullable = true)\n",
      " |-- OFFENSE_DESCRIPTION: string (nullable = true)\n",
      " |-- OFFENSE_INTERNAL_CODE: integer (nullable = true)\n",
      " |-- OFFENSE_INTERNAL_DESCRIPTION: string (nullable = true)\n",
      " |-- OFFENSE_RESULT: string (nullable = true)\n",
      " |-- OFFENSE_LEVEL: string (nullable = true)\n",
      " |-- JURISDICTION: string (nullable = true)\n",
      " |-- BOROUGH: string (nullable = true)\n",
      " |-- PRECINCT: integer (nullable = true)\n",
      " |-- SPECIFIC_LOCATION: string (nullable = true)\n",
      " |-- PREMISE_DESCRIPTION: string (nullable = true)\n",
      " |-- PARK_NAME: string (nullable = true)\n",
      " |-- HOUSING_NAME: string (nullable = true)\n",
      " |-- X_COORD_NYC: integer (nullable = true)\n",
      " |-- Y_COORD_NYC: integer (nullable = true)\n",
      " |-- LAT: double (nullable = true)\n",
      " |-- LON: double (nullable = true)\n",
      " |-- LAT_LON: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "oldColumns = df.schema.names\n",
    "newColumns = [\n",
    "    'COMPLAINT_NUMBER',\n",
    "    'COMPLAINT_START_DATE',\n",
    "    'COMPLAINT_START_TIME',\n",
    "    'COMPLAINT_END_DATE',\n",
    "    'COMPLAINT_END_TIME',\n",
    "    'REPORTED_DATE',\n",
    "    'OFFENSE_ID',\n",
    "    'OFFENSE_DESCRIPTION',\n",
    "    'OFFENSE_INTERNAL_CODE',\n",
    "    'OFFENSE_INTERNAL_DESCRIPTION',\n",
    "    'OFFENSE_RESULT',\n",
    "    'OFFENSE_LEVEL',\n",
    "    'JURISDICTION',\n",
    "    'BOROUGH',\n",
    "    'PRECINCT',\n",
    "    'SPECIFIC_LOCATION',\n",
    "    'PREMISE_DESCRIPTION',\n",
    "    'PARK_NAME',\n",
    "    'HOUSING_NAME',\n",
    "    'X_COORD_NYC',\n",
    "    'Y_COORD_NYC',\n",
    "    'LAT',\n",
    "    'LON',\n",
    "    'LAT_LON'\n",
    "]\n",
    "\n",
    "df = reduce(lambda data, idx: data.withColumnRenamed(oldColumns[idx], newColumns[idx]), range(len(oldColumns)), df)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop rows with any NA values in the specified columns\n",
    "df_na_drop = df.na.drop(subset=[\n",
    "    'COMPLAINT_START_DATE',\n",
    "    'COMPLAINT_START_TIME',\n",
    "    'OFFENSE_DESCRIPTION',\n",
    "    'OFFENSE_RESULT',\n",
    "    'BOROUGH',\n",
    "    'PRECINCT',\n",
    "    'LAT',\n",
    "    'LON'\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make the conversion of times here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace 24:00:00 with 00:00:00 in the time columns\n",
    "df_na_drop = df_na_drop.withColumn('COMPLAINT_START_TIME', regexp_replace('COMPLAINT_START_TIME', '24:00:00', '00:00:00'))\n",
    "df_na_drop = df_na_drop.withColumn('COMPLAINT_END_TIME', regexp_replace('COMPLAINT_END_TIME', '24:00:00', '00:00:00'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add table to SQL Context\n",
    "df_na_drop.createOrReplaceTempView(\"df_na_drop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query started at 10:19:53 PM UTC; Query executed in 0.55 m"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count(1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count(1)\n",
       "0         0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%read_sql\n",
    "SELECT count(*) FROM df_na_drop WHERE COMPLAINT_START_TIME = '24:00:00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query started at 10:20:27 PM UTC; Query executed in 0.53 m"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count(1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30887</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count(1)\n",
       "0     30887"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%read_sql\n",
    "SELECT count(*) FROM df_na_drop WHERE COMPLAINT_START_TIME = '00:00:00'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like it's worked to me. Let's continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_na_drop = df_na_drop.withColumn('COMPLAINT_END_DATE', coalesce(df_na_drop['COMPLAINT_END_DATE'], df_na_drop['COMPLAINT_START_DATE']))\n",
    "df_na_drop = df_na_drop.withColumn('COMPLAINT_END_TIME', coalesce(df_na_drop['COMPLAINT_END_TIME'], df_na_drop['COMPLAINT_START_TIME'])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Combine date and time fields and create new timestamp field for COMPLAINT fields\n",
    "df_na_drop = df_na_drop.withColumn(\n",
    "    'COMPLAINT_START_TIMESTAMP', \n",
    "    to_timestamp(\n",
    "        concat(df_na_drop['COMPLAINT_START_DATE'], lit(' '), df_na_drop['COMPLAINT_START_TIME']),\n",
    "        'MM/dd/yyyy HH:mm:ss'\n",
    "    )\n",
    ")\n",
    "\n",
    "df_na_drop = df_na_drop.withColumn(\n",
    "    'COMPLAINT_END_TIMESTAMP', \n",
    "    to_timestamp(\n",
    "        concat(df_na_drop['COMPLAINT_END_DATE'], lit(' '), df_na_drop['COMPLAINT_END_TIME']),\n",
    "        'MM/dd/yyyy HH:mm:ss'\n",
    "    )\n",
    ")\n",
    "\n",
    "# Convert REPORTED_DATE\n",
    "df_na_drop = df_na_drop.withColumn(\n",
    "    'REPORTED_DATE_TIMESTAMP', \n",
    "    to_timestamp(\n",
    "        df_na_drop['REPORTED_DATE'],\n",
    "        'MM/dd/yyyy'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# List of crimes to keep\n",
    "crimes_to_keep = [\n",
    "    'PETIT LARCENY',\n",
    "    'HARRASSMENT 2',\n",
    "    'ASSAULT 3 & RELATED OFFENSES',\n",
    "    'CRIMINAL MISCHIEF & RELATED OF',\n",
    "    'GRAND LARCENY',\n",
    "    'OFF. AGNST PUB ORD SENSBLTY &',\n",
    "    'DANGEROUS DRUGS',\n",
    "    'ROBBERY',\n",
    "    'BURGLUARY',\n",
    "    'FELONY ASSAULT'\n",
    "]\n",
    "\n",
    "# Anything not in the list becomes 'OTHER'\n",
    "df_na_drop = df_na_drop.withColumn(\n",
    "    'OFFENSE_DESCRIPTION', \n",
    "    when(df_na_drop['OFFENSE_DESCRIPTION'].isin(crimes_to_keep), df_na_drop['OFFENSE_DESCRIPTION']).otherwise('OTHER')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# List of premises to keep\n",
    "premises_to_keep = [\n",
    "    'STREET',\n",
    "    'RESIDENCE - APT. HOUSE',\n",
    "    'RESIDENCE-HOUSE',\n",
    "    'RESIDENCE - PUBLIC HOUSING',\n",
    "    'COMMERCIAL BUILDING',\n",
    "    'DEPARTMENT STORE',\n",
    "    'TRANSIT - NYC SUBWAY',\n",
    "    'CHAIN STORE',\n",
    "    'PUBLIC SCHOOL',\n",
    "    'GROCERY/BODEGA',\n",
    "    'RESTAURANT/DINER',\n",
    "    'BAR/NIGHT CLUB',\n",
    "    'PARK/PLAYGROUND'\n",
    "]\n",
    "\n",
    "# Anything not in the list becomes 'OTHER'\n",
    "df_na_drop = df_na_drop.withColumn(\n",
    "    'PREMISE_DESCRIPTION', \n",
    "    when(df_na_drop['PREMISE_DESCRIPTION'].isin(premises_to_keep), df_na_drop['PREMISE_DESCRIPTION']).otherwise('OTHER')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set UDFs to extract specific parts of date and time\n",
    "extract_year =  udf(lambda x: x.year)\n",
    "extract_month =  udf(lambda x: x.month)\n",
    "extract_day =  udf(lambda x: x.day)\n",
    "extract_hour =  udf(lambda x: x.hour)\n",
    "\n",
    "# Perform transformation\n",
    "df_na_drop = df_na_drop.withColumn('COMPLAINT_START_TIMESTAMP_YEAR', extract_year(col('COMPLAINT_START_TIMESTAMP')))\n",
    "df_na_drop = df_na_drop.withColumn('COMPLAINT_START_TIMESTAMP_MONTH', extract_month(col('COMPLAINT_START_TIMESTAMP')))\n",
    "df_na_drop = df_na_drop.withColumn('COMPLAINT_START_TIMESTAMP_DAY', extract_day(col('COMPLAINT_START_TIMESTAMP')))\n",
    "df_na_drop = df_na_drop.withColumn('COMPLAINT_START_TIMESTAMP_WEEKDAY', date_format(col('COMPLAINT_START_TIMESTAMP'), 'E'))\n",
    "df_na_drop = df_na_drop.withColumn('COMPLAINT_START_TIMESTAMP_HOUR', extract_hour(col('COMPLAINT_START_TIMESTAMP')))\n",
    "\n",
    "df_na_drop = df_na_drop.withColumn('COMPLAINT_END_TIMESTAMP_YEAR', extract_year(col('COMPLAINT_END_TIMESTAMP')))\n",
    "df_na_drop = df_na_drop.withColumn('COMPLAINT_END_TIMESTAMP_MONTH', extract_month(col('COMPLAINT_END_TIMESTAMP')))\n",
    "df_na_drop = df_na_drop.withColumn('COMPLAINT_END_TIMESTAMP_DAY', extract_day(col('COMPLAINT_END_TIMESTAMP')))\n",
    "df_na_drop = df_na_drop.withColumn('COMPLAINT_END_TIMESTAMP_WEEKDAY', date_format(col('COMPLAINT_END_TIMESTAMP'), 'E'))\n",
    "df_na_drop = df_na_drop.withColumn('COMPLAINT_END_TIMESTAMP_HOUR', extract_hour(col('COMPLAINT_END_TIMESTAMP')))\n",
    "\n",
    "df_na_drop = df_na_drop.withColumn('REPORTED_DATE_TIMESTAMP_YEAR', extract_year(col('REPORTED_DATE_TIMESTAMP')))\n",
    "df_na_drop = df_na_drop.withColumn('REPORTED_DATE_TIMESTAMP_MONTH', extract_month(col('REPORTED_DATE_TIMESTAMP')))\n",
    "df_na_drop = df_na_drop.withColumn('REPORTED_DATE_TIMESTAMP_DAY', extract_day(col('REPORTED_DATE_TIMESTAMP')))\n",
    "df_na_drop = df_na_drop.withColumn('REPORTED_DATE_TIMESTAMP_WEEKDAY', date_format(col('REPORTED_DATE_TIMESTAMP'), 'E'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Take the difference between start and end, expressed in minutes\n",
    "df_na_drop = df_na_drop.withColumn(\n",
    "    'COMPLAINT_LENGTH', \n",
    "    (unix_timestamp(df_na_drop['COMPLAINT_END_TIMESTAMP']) - unix_timestamp(df_na_drop['COMPLAINT_START_TIMESTAMP']))/60\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If COMPLAINT_LENGTH = 0, we flag with a new boolean column COMPLAINT_LENGTH_ZERO_TIME\n",
    "df_na_drop = df_na_drop.withColumn(\n",
    "    'COMPLAINT_LENGTH_ZERO_TIME', when(df_na_drop['COMPLAINT_LENGTH'] == 0, True).otherwise(False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Take the difference between start and reported, expressed in days\n",
    "df_na_drop = df_na_drop.withColumn(\n",
    "    'COMPLAINT_START_REPORTED_LAG', \n",
    "    (unix_timestamp(df_na_drop['REPORTED_DATE_TIMESTAMP']) - unix_timestamp(to_date(df_na_drop['COMPLAINT_START_TIMESTAMP'])))/60/60/24\n",
    ")\n",
    "\n",
    "# Take the difference between end and reported, expressed in days\n",
    "df_na_drop = df_na_drop.withColumn(\n",
    "    'COMPLAINT_END_REPORTED_LAG', \n",
    "    (unix_timestamp(df_na_drop['REPORTED_DATE_TIMESTAMP']) - unix_timestamp(to_date(df_na_drop['COMPLAINT_END_TIMESTAMP'])))/60/60/24\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, let's check the dataframe for null timestamp values again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add table to SQL Context\n",
    "df_na_drop.createOrReplaceTempView(\"df_na_drop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query started at 10:21:00 PM UTC; Query executed in 0.71 m"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count(1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count(1)\n",
       "0         0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%read_sql\n",
    "SELECT COUNT(*) FROM df_na_drop WHERE COMPLAINT_START_TIMESTAMP IS NULL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay let's try saving as a parquet again to S3. Wait, we haven't gone over parquets! Yet another term to learn for better or for worse. Let's pretend here like it's for the better haha.\n",
    "\n",
    "A parquet is a different method of storing a file. While CSV is raw text delimited by a comma, a parquet is a much more intricate and complex data storage algorithm.\n",
    "\n",
    "Its advantage is that its a _**columnar**_ storage format, which means that each column is stored separately, rather than each row (think about when you open up a CSV, you generally read it by _**rows**_. Only when you format the data into a Python array or a Python / Pyspark dataframe can you really start referring to it by columns. This gives us storage and speed disadvantages.\n",
    "\n",
    "The _**storage**_ advantages come from the fact that each column now has a specific context. Just like how a Pyspark dataframe column can be designated as a string or float or timestamp, parquets have overhead and metadata within the file that make this distinction as well, again, because the data is being stored in a _**columnar**_ format. Even further, this then allows us to actually _**encode and compress**_ columns in specific ways, gaining efficiencies where possible.\n",
    "\n",
    "The _**speed**_ advantages, gain, stem from the columnar format and builds on top of the storage advantages as well. When querying data, often, we only want to select one or two columns, right? The columnar storage format actually allows us to only scan specific columns and, therefore, we don't need to scan the entire file like we would if we loaded up an entire CSV! The fact that the data is compressed also implies that we end up scanning less raw binary (with some overhead to uncompress, but compute power is much faster than disk reading).\n",
    "\n",
    "A hidden advantage here is that my _**AWS storage & data transfer**_ costs should go down as well!\n",
    "\n",
    "This is all I know about parquet for now, and I won't really be taking advantage of anything other than the compression abilities of parquet right now (I will still end up loading the entire parquet to Spark working memory and slicing and dicing from there), but good to know for the future as AWS is starting to build out services standardized on parquets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_clean = df_na_drop[[\n",
    "    'COMPLAINT_NUMBER', \n",
    "    'COMPLAINT_START_TIMESTAMP',\n",
    "    'COMPLAINT_END_TIMESTAMP',\n",
    "    'REPORTED_DATE_TIMESTAMP',\n",
    "    'COMPLAINT_START_TIMESTAMP_YEAR',\n",
    "    'COMPLAINT_START_TIMESTAMP_MONTH',\n",
    "    'COMPLAINT_START_TIMESTAMP_DAY',\n",
    "    'COMPLAINT_START_TIMESTAMP_WEEKDAY',\n",
    "    'COMPLAINT_START_TIMESTAMP_HOUR',\n",
    "    'COMPLAINT_END_TIMESTAMP_YEAR',\n",
    "    'COMPLAINT_END_TIMESTAMP_MONTH',\n",
    "    'COMPLAINT_END_TIMESTAMP_DAY',\n",
    "    'COMPLAINT_END_TIMESTAMP_WEEKDAY',\n",
    "    'COMPLAINT_END_TIMESTAMP_HOUR',\n",
    "    'REPORTED_DATE_TIMESTAMP_YEAR',\n",
    "    'REPORTED_DATE_TIMESTAMP_MONTH',\n",
    "    'REPORTED_DATE_TIMESTAMP_DAY',\n",
    "    'REPORTED_DATE_TIMESTAMP_WEEKDAY',\n",
    "    'COMPLAINT_LENGTH',\n",
    "    'COMPLAINT_LENGTH_ZERO_TIME',\n",
    "    'COMPLAINT_START_REPORTED_LAG',\n",
    "    'COMPLAINT_END_REPORTED_LAG',\n",
    "    'OFFENSE_DESCRIPTION',\n",
    "    'OFFENSE_RESULT',\n",
    "    'OFFENSE_LEVEL',\n",
    "    'JURISDICTION',\n",
    "    'BOROUGH',\n",
    "    'PRECINCT',\n",
    "    'SPECIFIC_LOCATION',\n",
    "    'PREMISE_DESCRIPTION',\n",
    "    'LAT',\n",
    "    'LON'\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save CSV back to S3\n",
    "df_clean.write.parquet('s3n://2017edmfasatb/nypd_complaints/data/df_clean.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hurrah! It worked. Our file is now stored on S3. It's pretty interesting. Our file is actually a folder consisting of multiple parts. This is because Spark is saving to a file within Spark takes into account partitioning abilities! Our partitions look like this:\n",
    "\n",
    "<img src=\"https://s3.ca-central-1.amazonaws.com/2017edmfasatb/nypd_complaints/images/28_parquet_aws.png\" width=\"700\">\n",
    "\n",
    "This sums to a grand total of... _**120MB**_. Good for a compression of _**91%**_... This doesn't even seem real, but I will absolutely not complain for now. Talk about storage gains... sheesh.\n",
    "\n",
    "Anyways, our file is finally stored. Let's actually continue onto actual anaylsis-oriented exploration!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
